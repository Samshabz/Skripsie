

\chapter{System Design}

This chapter outlines the chosen methodology for the UAV navigation system, detailing the pipeline and its various components. The system is designed to accurately estimate the UAV's position and heading, even in scenarios where GPS data is unreliable or unavailable. The high-level flow of the system is illustrated in Figure \ref{fig:HighLevelFlow}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Chapter 3/Chap3Figs/HighLevelFlow.png}
    \caption{High-Level Flow of the System}
    \label{fig:HighLevelFlow}
\end{figure}




\section{System Pipeline}

The UAV image-processing pipeline comprises several stages, each addressing specific goals in the larger aim to estimate the new GPS and heading of the UAV. These stages require varying levels of precision and efficiency to ensure robust performance.

\subsection{Pipeline Stages}

\begin{enumerate}
    \item \textbf{Image Input:}  
    The process begins with capturing a live image from the UAV’s downward-facing camera. This real-time visual input provides essential information about the UAV’s environment, forming the basis for position estimation.

    \item \textbf{Feature Extraction:}  
    Keypoints and descriptors are extracted from the current image to aid in the matching process. This extraction occurs in two layers:
    \begin{enumerate}
        \item \textbf{Coarse Layer:} Used for proximity search space reduction.
        \item \textbf{Dense Layer:} Used for precise matching and transformation estimation. 
    \end{enumerate}
    By also performing feature extraction when GPS is available, the system reduces computational load during the critical phase when GPS is lost. 

    \item \textbf{Storage (Telemetry and Features):}  
    Extracted features (keypoints and descriptors) along with telemetry data (GPS position and heading) are stored for future reference. Stored features facilitate relative transformation inference when GPS is unavailable, while telemetry data assists in converting relative transformations to real-world coordinates and headings.

    \item \textbf{Match Features:}  
    Features between the current image and reference images are matched to ensure that comparisons are based on mutual features. This involves two matching stages:
    \begin{enumerate}
        \item \textbf{Coarse Matching:} Matches the coarse layer of features to reduce the search space.
        \item \textbf{Dense Matching:} Matches the dense layer for precise position estimation.
    \end{enumerate}
    The matching process is optimized to ensure robustness against noise and outliers, with a focus on computational efficiency. The techniques are as follows:
    \begin{enumerate}
        \item \textbf{Feature Matching:}  
        Utilizes FLANN and BruteForce matchers to identify potential matches between the current and reference images. The matchers are optimized to ensure robustness against noise and outliers, with a focus on computational efficiency.
        
        \item \textbf{Optimization:}  
        Refines the matches using techniques such as Lowe's ratio test to ensure that only high-quality matches are considered for further processing. This refinement step is crucial for accurate transformation estimation. 
    \end{enumerate}
    \item \textbf{Similarity Comparison:}  
    The system compares the input image with reference images to identify the most similar one. This comparison is conducted using global matching techniques on aligned images to ensure efficiency and accuracy.
    \begin{enumerate}
        \item \textbf{Proximity Search Space Reduction:}  
        Reduces the search space based on proximity from the last known or estimated GPS location. A fixed radius was initially used, but later updated to return a specific number of the closest matches. 
        
        \item \textbf{Rotational Alignment:}  
        Estimates the rotation between the input and reference images using the coarse layer of features. The images are aligned using the course estimate as global matchers tolerate minor rotational inaccuracies well.
        
        \item \textbf{Best Match Identification:}  
        Computes similarity scores between the input image and each candidate image using global matching techniques. The image with the highest similarity score is selected as the best match, serving as the reference for position estimation.
    \end{enumerate}



    \item \textbf{Planar Transformation Estimation:}  
    After identifying the best match, the system performs a precise estimation of both rotation and translation between the input and reference images using the dense match layer. This involves several sub-steps to enhance accuracy:
    \begin{enumerate}
        \item \textbf{Angle Estimation:}  
        Utilizes the chosen transformation method to obtain a precise estimate of the rotation between the input and reference images, forming the basis for heading estimation.
        
        \item \textbf{Image Alignment \& Recomputation of Dense Layer:}  
        Aligns the input image with the reference image based on the estimated rotation, thereby implicitly removing non-mutual information by rotating it off the canvas. The dense layer of features is recomputed on the aligned images to ensure accurate translation estimates.
        
        \item \textbf{Translation Estimate:}  
        Performs a precise estimation of translation between the two images using the refined dense layer, providing the basis for GPS inference.
    \end{enumerate}

    \item \textbf{Update to Global Coordinate System:}  
    The estimated translation, initially in the internal image coordinate system (pixels), must be converted to real-world coordinates (metres) and then to the global coordinate system (longitude and latitude). This conversion involves several stages:
    \begin{enumerate}
        \item \textbf{Heading Update:}  
        Adds the estimated rotation to the reference image's heading to determine the UAV's new heading.
        
        \item \textbf{Translation Rotation from Internal to Global Coordinate System:}  
        Rotates the translation vector by the UAV's estimated global heading to align it with the global coordinate system without altering its magnitude.
        
        \item \textbf{Pixel to Metres (Relative):}  
        Converts the estimated change in pixels to metres using the dynamically inferred conversion factor. 
        
        Translates the metre-based changes to relative changes in longitude and latitude using the following equations: 

        \(\Delta \text{Longitude} = \frac{\Delta \text{Metres}}{111320 \times \cos(\text{Latitude})}\), \(\Delta \text{Latitude} = \frac{\Delta \text{Metres}}{111320}\)

        These calculations account for the Earth's oblate spheroid shape, ensuring accurate positional data by adjusting for the decreasing distance between longitudes as one moves towards the poles.
        
        \item \textbf{Conversion to Absolute GPS:}  
        Adds the relative changes in longitude and latitude to the GPS coordinates of the reference image, resulting in the UAV's new GPS position.
    \end{enumerate}

    \item \textbf{Output New GPS and Heading:}  
    The calculated translation and rotation values are used to estimate the UAV's new GPS coordinates and heading. This enables pilots to navigate the UAV back to its base along the original path, even in the absence of GPS.

    \item \textbf{Dynamic Parameter Inference:}  
    This stage is performed while GPS is available. It estimates the GPS position using a temporary pixel-to-metre conversion factor of 1, utilizing the pipeline's stages to compute this estimate. The estimated GPS is compared to the ground truth GPS, and the difference is used to adjust the pixel-to-metre conversion factor. This ensures accurate conversion across datasets with varying altitudes. The first five images are used to infer this scaling factor.


\end{enumerate}


\section{Dynamic Methods and Techniques}
Upon rigorous testing, it was seen that many parameters in the pipeline simply cannot generalize without tuning. This included the parameters of some feature detectors, optimization techniques, and search space reduction methods. To address this, dynamic methods were implemented to adjust these parameters based on the current scenario. The following dynamic methods were implemented:

% begin itemize
\begin{itemize}
    \item \textbf{Dynamic Feature Detector Selection:} AKAZE, the only detector without a dedicated keypoint target parameter (such as ORB's nFeatures), was highly accurate when tuned. However, the leniency threshold of AKAZE was found to be inconsistent across datasets. To address this, a dynamic method was implemented to adjust the leniency threshold based on the number of keypoints detected. This ensures that the detector is neither too lenient nor too strict, providing a balance between accuracy and efficiency.
    \item \textbf{Lowe's Ratio:} Lowe's ratio test is a critical step in the matching process, ensuring that only high-quality matches are considered for further processing. However, the optimal threshold for this test varies highly across datasets and scenarios. To address this, a dynamic method was implemented to adjust the Lowe's ratio threshold, starting at a strict threshold, and gradually relaxing it until a sufficient number of matches are found. This ensures that the system remains robust against noise and outliers while maintaining computational efficiency.
\end{itemize}

The above dynamic methods ensure that the system remains stable in different scenarios, adapting to varying conditions to provide accurate and efficient performance. However, these methods still, in part, fail to understand the nuances of the dataset perfectly. In the future, in flight error to ground truth estimates can be used to iteratively solve for the most optimal static parameters for the given environment, assuming it does not significantly change during flight. Alternatively, a dedicated machine learning model can be trained to predict and constantly adjust the optimal parameters based on the dataset's characteristics in-flight, providing a more robust and efficient solution.


\section{Rotational Normalization Strategies}


Rotational normalization is crucial for accurately aligning images internally and to the global heading space to estimate the UAVs relative and absolute positions. However, this inherently discards information that is rotated off of the canvas. Two primary strategies were evaluated: pre-normalization, which aligns images to a global North-East (NE) reference before keypoint detection, and post-normalization, which aligns images relative to each other before applying normalization after translation estimation. 

In the pre-normalization approach, information loss scales sinusoidally with the absolute values of the difference in rotations between images. Further, this loss contains predominantly losses in non-mutual pixels due to its internal alignment. Conversely, post-normalization scales with both angle losses independently, increasing the likelihood of compounded loss. 

Given that the UAV's typical return flight will be 180 degrees to the outbound path, with a maximum path deviation of ±20 degrees, the loss functions for the two methods are illustrated in Figure~\ref{fig:lossprevpost}. This function shows losses exceeding 2.0 due to the compounded effect of independent image normalization, whereas the post-normalization method maintains losses around 0.35 by preserving keypoint information more effectively. 
Therefore, and in accordance with empirical testing results, post-normalization was adopted for its increased mutual information retention rates.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Chapter 4/Figs4/lossprevspost.png}
    \caption{Loss comparison between pre-normalization and post-normalization methods.}
    \label{fig:lossprevpost}
\end{figure}






\section{Testing Shortlist}
\label{sec:testing_shortlist}

The system design integrates multiple features to ensure robustness, accuracy, and efficiency in UAV navigation tasks. The following components have been selected and tested across various scenarios:

\begin{itemize}
    \item \textbf{Feature Detectors:} AKAZE, SuperPoint (with LightGlue matcher), and ORB. Note, the SuperPoint detector is used in conjunction with the LightGlue matcher to enhance performance.
    \item \textbf{Feature Matchers:} FLANN and BruteForce. KNN 
    \item \textbf{Search Techniques:} KNN with \( K=2 \). Empirical tests showed a value of 1 to be ineffective without Lowe's ratio test, while values above 2 introduced excessive computational overheads. Further, radius search was seen to be unreliable due to the varying density of keypoints across datasets.
    \item \textbf{Planar Transformation Estimation:} Homography, Affine, Partial Affine (Rigid) transformations using OpenCV, and SVD-based rigid transformations for rotation and translation estimations. Rotational and Translational estimates were initially tested separately, due to the possibility of different responses to different prior stages and methods. However, it was seen that responses were comparatively equivalent; the methods were tested using a combined transform.
    \item \textbf{Global Image Similarity Measures:} SSIM, histogram matching, local retrofit, and cross-correlation.
    \item \textbf{Optimization Techniques:} Standard Deviation Filtering, LMEDS, RANSAC, Lowe's ratio test, n-Match thresholding, and absolute thresholding for match refinement. Empirical tests indicated that cross-checking was not applicable due to the excessive computational costs involved. KNN
\end{itemize}


\section{Conclusion}

The system design integrates feature extraction, matching, and transformation estimation to enable precise UAV navigation, even in the absence of reliable GPS data. By employing a combination of diverse feature detectors and optimizing various pipeline stages, the system ensures robustness, accuracy, and computational efficiency across different operational scenarios. 

