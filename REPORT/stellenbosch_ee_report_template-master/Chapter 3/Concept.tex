\section*{Concept}

The flow of the camera system shall be as follows:

\begin{enumerate}
    \item The controller shall receive footage in real-time from the UAV and sync it with known telemetry data such as GPS coordinates, altitude, heading and speed.
    \item The controller shall then process the video footage and telemetry data to extract and store the relevant features
    \item When the GPS signal is lost, the UAV will turn around
    \item The controller shall extract and match the features of the current image with that of the previous images
    \item It will find the most correlated images and use that image to determine the change in feature locations to infer the UAV's position
\end{enumerate}

\section*{Feature Extractors}
these are the extractors
orb, surf, sift, akaze, brisk
deep learning extractors: superpoint, d2-net, r2d2

This study will introduce the most popular and state-of-the-art feature detectors. There will be rough research as to why these extractors were chosen. Where possible, we will compare methods and remove superceded methods. Thereafter, we will consider the methods which are most likely to work for the given application. The study will test these methods and compare them to determine the best method for the given application. The study will consider primarily the accuracy of the method, in terms of its error. It will also consider the computational cost in cases where it does not meet the semi-real-time requirements or where accuracy is similar. 


\subsection*{SIFT (Scale-Invariant Feature Transform)}
SIFT detects keypoints using the Difference of Gaussians (DoG), where Gaussian-blurred images at multiple scales are subtracted to highlight significant intensity changes. Keypoints are identified as maxima or minima in the DoG and are stored with their scale information. SIFT then assigns an orientation by analyzing gradients around each keypoint, ensuring rotation invariance. The final 128-dimensional descriptor encodes gradient information relative to the keypoint’s orientation and scale, making it robust to changes in scale, rotation, and illumination.
NOT USED NOT FREE

\subsection*{SURF (Speeded-Up Robust Features)}
SURF detects keypoints using the Hessian matrix, which measures image curvature to find corners and edges. To speed up this process, SURF uses box filters and integral images, allowing quick computation. Keypoints are detected where the Hessian determinant is highest. SURF assigns an orientation using Haar wavelets, which measure intensity changes in the horizontal and vertical directions around the keypoint. The keypoint descriptor is built by summing wavelet responses within a square region, creating a compact 64-dimensional (or 128-dimensional) descriptor for feature matching.
Good balance of speed and robustness, suitable for real-time mapping.

\subsection*{ORB (Oriented FAST and Rotated BRIEF)}
ORB detects keypoints using the FAST algorithm, which identifies corners by analyzing a circle of pixels around each point. The keypoints are ranked using the Harris corner measure, and only the strongest are kept. ORB assigns an orientation to each keypoint by calculating the direction of the intensity gradient, ensuring rotation invariance. The descriptor is a modified version of BRIEF, which compares pixel intensities within the keypoint's neighborhood. The pattern is rotated according to the keypoint's orientation, resulting in a 256-bit binary descriptor that is fast and efficient for real-time applications.
Fast and efficient, suitable if you need quick results with moderate accuracy.


\subsection*{AKAZE (Accelerated-KAZE)}
AKAZE detects keypoints in a nonlinear scale space, created using nonlinear diffusion filtering, which preserves edges better than traditional Gaussian blurring. Keypoints are found by locating extrema in this scale space. Each keypoint is assigned an orientation based on the local gradient direction, ensuring rotation invariance. The descriptor, M-LDB, is a binary string formed by comparing intensity differences between pairs of points around the keypoint. AKAZE is robust to varying conditions and computationally efficient.
Best balance of accuracy and robustness for complex landscapes.


\subsection*{BRISK (Binary Robust Invariant Scalable Keypoints)}
BRISK detects keypoints using a scale-space pyramid, applying the FAST detector at multiple scales. Keypoints are refined through non-maximal suppression to retain only the strongest. Orientation is assigned by analyzing the intensity gradients around the keypoint. The descriptor is a binary string generated by comparing pixel intensities in a circular pattern, resulting in a 512-bit descriptor. BRISK is designed for speed and is effective in scenarios with scale and rotation variations.
Fastest, but less robust, better for simpler or less variable environments.


\subsection*{Others}
Feature detectors need to be paired with descriptor extractors to be useful. The above do both. There are also individual feature detectors and descriptor extractors which would be paired with either the above methods or ones below. Below, they are analysed. 
Pure feature detectors (keypoints no descriptors)
STAR is used for blob-like detection although not very robust and usable for mapping. 
FAST, purely an extractor, not a descriptor. not very robust, but very fast.
MSER, niche applications like text detection. Not very robust or fast.
GFTT Excellent for motion tracking but lacks robustness in scale and rotation invariance needed for UAV mapping.
Harris-corner detector: Effective in stable environments with well-defined corners, but limited by its lack of scale and rotation invariance
Dense: More suited to dense sampling in texture or object recognition tasks, not specifically useful for detecting DISTINCTIVE features for mapping.
SimpleBlobDetector: Best in environments with simple, blob-like features but not ideal for general UAV mapping tasks


Feature descriptors (no keypoints):
BRIEF generates binary descriptors by comparing pixel intensities in a fixed pattern around each keypoint. It is fast but not scale or rotation invariant

FREAK: similar to brief in efficiency and a lack of scale invariance, however, it is rotation invariant. Once again, not suitable for UAV mapping.

Given the focus on efficiency of both of these methods, it is not necessary to pair them with any of the above methods as the accuracy, our main consideration is likely to be lower than the above methods.






\section*{Comparison of Feature Detectors} 

%https://mikhail-kennerley.medium.com/a-comparison-of-sift-surf-and-orb-on-opencv-59119b9ec3d0



\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Feature Detector} & \textbf{ORB} & \textbf{SURF} & \textbf{SIFT} \\ \hline
\textbf{Time to Compute 300 Keypoints (ms)} & 15 & 115 & 120 \\ \hline
\textbf{Average Total Number of Keypoints per Image} & 7300 & 2300 & 1800 \\ \hline
\textbf{Matching Percentage Given Illumination Changes} & 96\% & 89\% & 88\% \\ \hline
\textbf{Matching Percentage Given Rotation Changes} & 100\% & 100\% & 92\% \\ \hline
\end{tabular}
\end{table}

The given study indicates that ORB is the fastest, it ties for first with SURF in terms of rotation invariance, and it is the best at handling illumination changes. For our application, rotation and illumination invariance are critical. The speed of the detector is also important as the UAV will be processing images in real-time. ORB is the best choice for this application. Second to ORB is SURF, followed by SIFT, however more study will be conducted to determine the better between the two



Comparison of sift and surf







in practice sift better surf at scale inv, surf better at rot inv, warping, blur. both good for illumination

% insert image of the comparison of the detectors, its called SIFT_SURF_COMP.jpg
%LINK: https://www.researchgate.net/publication/314285930_Comparison_of_Feature_Detection_and_Matching_Approaches_SIFT_and_SURF

\begin{table}[H]
    \centering
    \caption{Comparison of SIFT and SURF for different image transformations}
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Image Transformation} & \textbf{SIFT Matching Pairs} & \textbf{SURF Matching Pairs} \\ \hline
    Rotate 90°                    & 215                          & 309                          \\ \hline
    Different Scale               & 153                          & 32                           \\ \hline
    Blur                          & 29                           & 217                          \\ \hline
    Different Hue                 & 153                          & 169                          \\ \hline
    Different Saturation/Value    & 49                           & 220                          \\ \hline
    Warp                          & 266                          & 565                          \\ \hline
    RGB Noise                     & 23                           & 57                           \\ \hline
    \end{tabular}
    \label{tab:sift_surf_comparison}
    \end{table}
    


The above is a comparison of the invariance of SIFT and SURF. It tests multiple distortion types as well as execution time, and the number of keypoints detected. SIFT performs better in terms of scale invariance, while SURF performs better in terms of rotation invariance, warping, blur and marginally in illumination changes. This study was limited in terms of the number of images used and scope of testing. However, it provides a good indication of which feature detector is likely to work better for the given application. UAVs are likely to experience rotation changes and illumination changes on the reverse flight. The scope of this study is limited to relatively stable altitudes, and hence scale invariance is not as important. SURF is likely than SIFT to offer superior performance in this application. Due to scope constraints, the study will not test SIFT in practice. 






\section*{Deep-Learning Based Feature Detectors}

Deep learning-based feature detectors have gained popularity in recent years due to their ability to learn extremely discriminative features directly from data. This study makes use of pre-trained detectors to maintain scope. Some popular deep learning-based feature detectors include SuperPoint, R2D2, and D2-Net. These detectors offer high performance and robustness to a large number of transformations, making them suitable for precise and challenging applications. 
There are two limitations for these models. Firstly, they require a significant amount of matrix multiplication, which is computationally expensive and requires a GPU. Secondly, the pre-trained data is unlikely to have included images of landscapes akin to that of the UAV flight path. This means that the model may not be able to generalize well to the given application. In future work, it may be beneficial to train a model on the specific dataset to improve performance.

xxx - choose slam or homography later
\subsection*{Superpoint (Self-Supervised Interest Point Detection and Description - 2018)}
SuperPoint is an end-to-end neural network designed for keypoint detection and description, trained on synthetic data to perform these tasks simultaneously. The network consists of two stages: a base CNN that processes the image to produce a dense feature map and a head that detects keypoints from this map. The keypoints are then assigned descriptors by sampling the surrounding feature map. SuperPoint is unique in that it learns both the keypoint locations and the descriptors in a fully supervised manner, optimizing them for tasks like visual SLAM and image matching. The keypoints detected by SuperPoint are highly repeatable, and the descriptors are robust, making the method effective for various computer vision applications. Despite being more computationally efficient than other deep learning methods, SuperPoint still requires considerable resources compared to traditional approaches, making it more suitable for scenarios where the computational cost is justified by the need for high accuracy and robustness.


D2-Net (2019)
D2-Net is a deep learning-based approach that integrates keypoint detection and description into a single process using a Convolutional Neural Network (CNN). The network processes the input image to generate feature maps, which are used to identify keypoints by finding local maxima in these maps. This step ensures the keypoints are robust and scale-invariant, as they are derived from multiple scales within the network. For each detected keypoint, a high-dimensional descriptor is generated by sampling a patch of the feature map around the keypoint, capturing the local structure of the image. D2-Net is particularly strong in handling significant geometric transformations like rotation, scale changes, and perspective distortions, making it highly reliable for tasks requiring robust feature matching. However, due to the complexity of the network and the feature maps, D2-Net is computationally demanding, requiring significant processing power and memory.

R2D2 (2019)
R2D2 (Repeatable and Reliable Detector and Descriptor) focuses on detecting keypoints that are both repeatable and reliable across different images and conditions. It uses a neural network to produce a dense map of keypoints, where each keypoint is scored based on its reliability—how distinct it is compared to others—and its repeatability—how consistently it can be detected across varying views. The network optimizes for these properties, ensuring that the detected keypoints are both stable and distinctive. The descriptors in R2D2 are generated by sampling the feature map around each keypoint, similar to D2-Net, but with a focus on maintaining robustness to noise and ensuring distinctiveness. This makes R2D2 highly effective in challenging environments with significant changes in viewpoint, lighting, or partial occlusion. The approach, while accurate, requires more computational resources compared to traditional methods due to the deep learning-based processing.




These are the deep-learning extractors. DELF and LF-NET are also popular, however, slightly older and have been surpassed in most aspects by the following: (xxx - cite this)

D2-Net: Best for handling large transformations and providing dense, accurate keypoints and descriptors.
R2D2: Excellent for repeatable and reliable keypoints, especially in challenging conditions.
SuperPoint: Offers a good balance of accuracy and efficiency, with strong performance in real-time applications.




