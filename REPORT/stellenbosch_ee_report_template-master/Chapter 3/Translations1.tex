Estimating the translation between two images is crucial, as it directly infers the GPS. Initially, all images must be transformed into a global space to perform translation estimation. This ensures that the x and y coordinates are aligned with the North (N) and East (E) directions, respectively. This is done in the rotational alignment phase, where the reference image is perfectly aligned using telemetry data, and the inference image alignment is estimated. In the below methods, excluding the phase correlation method, the intra-image rotation is re-estimated. This step is considered redundant and will have a negligible effect on alignment. 

\subsection*{Primary Methods for Estimating Transformations}
In all methods, the inference and reference points (or images) are normalized to the global heading space. 

The difference in Degrees of Freedom (DoF) between methods is crucial. Higher DoF allows for more complex transformations but may introduce more error points in simpler cases. The data used for testing involves changes in rotation, translation and perspective to a smaller extent due to the flight altitude. 

\subsubsection*{Phase Correlation Method}
This method uses OpenCV's `cv2.phaseCorrelate` function to estimate the translation between two images. The images are pulled and aligned to the global heading space. Phase correlation computes the shift between the two images, yielding translation values along the E and N axes.  
Degrees of Freedom: 2 (translation in x and y directions). This method is ideal for estimating pure translation, but it doesn't handle rotation or scaling, making it suitable when the images are already well-aligned.

\subsubsection*{Direct Source Normalization with Rotation Correction}
This method normalizes the source and destination points by centering them around their means. It uses Singular Value Decomposition (SVD) to estimate the rotation between the points. The translation shift is calculated by averaging the differences between the destination points and the source points.  
Degrees of Freedom: 3 (translation in x and y, and rotation). 

\subsubsection*{Affine Transformation with RANSAC}
Affine transformation, estimated using OpenCV's `cv2.estimateAffine2D`, accounts for translation, rotation, and scaling. RANSAC is applied to filter out outliers, making the transformation estimation more robust.  
Degrees of Freedom: 6 (translation in x and y, rotation, scaling in x and y, shear). The high DoF allows for greater flexibility, handling complex distortions but may introduce extra error points in simpler cases.

\subsubsection*{Rigid Transformation Estimation (SVD)}
This method uses SVD to estimate a rigid transformation, preserving the shape of the object while calculating optimal rotation and translation. After centering the points, SVD determines the rotation matrix, followed by translation.  
Degrees of Freedom: 3 (translation in x and y, and rotation).

\subsubsection*{Homography Transformation}
Homography estimates a transformation that maps points between two planes, accounting for translation, rotation, scaling, and perspective distortion. OpenCV's `cv2.findHomography` calculates the homography matrix, from which translation and rotation components are extracted.  
Degrees of Freedom: 8 (translation, rotation, scaling, and perspective changes in x and y). 

\subsection*{Improvements for Transformation Estimation}

\subsubsection*{RANSAC for Affine Transformation}
RANSAC is used to improve the robustness of affine transformation estimation by iteratively selecting random subsets of points and fitting an affine model. This filters out outliers, ensuring only the most reliable matches contribute to the final transformation matrix.  
Degrees of Freedom: 6 (translation in x and y, rotation, scaling in x and y, shear). 

