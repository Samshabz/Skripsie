
\subsection*{Feature Extractors}


\subsubsection*{What is a feature Extractor}
A feature extractor is a computer vision algorithm or deep-learning model that identifies and extracts key features in an image. A feature is a pixel or weight of multiple pixels that represents a highly unique and distinctive part of an image. For instance, this could be an edge between a wall and the sky. Feature extractors are characterized by a spatial coordinate and a descriptor. A descriptor is a vector which contains information about the helps it be uniquely identified again in a new image. This can include information about the size, shape, and intensity of the feature. One can also infer the scale and orientation of the feature from the descriptor. A feature extractor also commonly includes a confidence score which indicates a score of how unique and well-defined the feature is. Ultimately, a feature extractors goal is to find unique and well-defined features that can be used to develop a complex understanding of the nuances and details of an image.
\subsubsection*{Usage in this task}
In this task, feature extractors are used to extract keypoints in images. These are used for intra-image rotational and translational estimates, and matching similar images. 



feature extractors are nb bc they r iinvariant to scale rotation and transformation. As opposed to simply correlating entire image. also less computationally expensive.
https://baotramduong.medium.com/feature-extraction-in-computer-vision-using-python-358d7c9863cb



common feature extraction techniques include edge detection, color histograms, and texture analysis



\section{Choosing a Feature Extractor}
When selecting a feature extractor, several critical factors must be considered:
\begin{itemize}
    \item \textbf{Accuracy:} The extractor needs to be accurate to ensure GPS inference is accurate. That is, the extractor should provide a sufficient quality and quantity of keypoints. This is quantified as subtending over 500 good matches from similar image pairs. 
    \item \textbf{Speed:} Be capable of real-time processing to ensure timely navigation and decision-making in dynamic environments. Specifically, initial tests must extract features in less than 1 second on a CPU. For the neural network-based models, the time taken to extract features must be less than 3 second on a CPU since they may be implemented with a GPU and CUDA libraries to improve performance. 
    \item \textbf{Robustness:} The feature extractor should exhibit invariance to changes in scale, rotation, illumination, perspective and noise to ensure accurate and repeatable performance across different flight datasets and conditions.
\end{itemize}

Feature extractors have different parameters trading-off accuracy and speed. For example, there are detection thresholds, descriptor sizes, and keypoint densities that can be adjusted to optimize performance. 

\section{Types of Feature Extractors}
\subsection{Traditional Feature Extractors}
\subsubsection{SIFT (Scale-Invariant Feature Transform)}
SIFT detects and describes local features in images. It is robust to changes in scale, rotation, and illumination, making it a reliable choice for many applications. However, its computational intensity can be a drawback in real-time scenarios.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy and robustness due to its multi-scale approach and precise keypoint localization. SIFT's descriptors are highly distinctive, enabling reliable matching across different views and conditions.
    \item \textbf{Disadvantages:} High computational cost and slower processing speed due to the extensive keypoint detection and descriptor computation steps, making it less suitable for real-time applications.
\end{itemize}


\subsubsection{SURF (Speeded-Up Robust Features)}
SURF is a faster alternative to SIFT, utilizing integral images for rapid computation of image convolutions. It offers good accuracy and robustness while being computationally more efficient than SIFT.
\begin{itemize}
    \item \textbf{Advantages:} Faster than SIFT due to its use of Haar wavelets and integral images, providing good balance between speed and accuracy. It maintains robustness to scale and rotation changes.
    \item \textbf{Disadvantages:} Still relatively computationally expensive compared to simpler methods like ORB, and can be less accurate than SIFT in certain complex scenarios.
\end{itemize}



\subsubsection{ORB (Oriented FAST and Rotated BRIEF)}
ORB combines the FAST keypoint detector and the BRIEF descriptor, providing a highly efficient feature extraction method suitable for real-time applications. It is designed to be both fast and invariant to rotation and scale.
\begin{itemize}
    \item \textbf{Advantages:} High speed and efficiency, making it suitable for real-time applications. ORB's binary descriptors are computationally less intensive while providing sufficient discriminative power for many tasks.
    \item \textbf{Disadvantages:} Lower accuracy compared to SIFT and SURF, especially in complex scenes with significant variations in lighting and scale. The binary nature of BRIEF descriptors can sometimes lead to higher false match rates.
\end{itemize}

\subsection{AKAZE (Accelerated-Keypoint-Affine-Zernike)}
AKAZE is a feature extraction method that combines speed and accuracy by using nonlinear scale space and feature detection. It is designed to be robust to various transformations and lighting conditions.
\begin{itemize}
    \item \textbf{Advantages:} High speed and efficiency due to its nonlinear scale space and feature detection approach. AKAZE is robust to scale, rotation, and illumination changes, making it suitable for diverse environments.
    \item \textbf{Disadvantages:} May not be as accurate as SIFT or SURF in certain scenarios, particularly those with complex textures or repetitive patterns. The trade-off between speed and accuracy may vary depending on the application.
\end{itemize}

\subsection{Deep Learning-Based Feature Extractors}
Deep learning-based feature extractors leverage neural networks to learn feature representations directly from data. These models need to be trained on large datasets to capture complex and hierarchical features effectively. They offer high accuracy and the ability to adapt to specific tasks through transfer learning.
    \item \textbf{Advantages:} High accuracy and the ability to learn complex and hierarchical features directly from data, enabling robust performance across diverse tasks and conditions. 
    \item \textbf{Disadvantages:} Requires substantial computational resources for training and inference. The training process is data-intensive, requiring large labeled datasets to achieve optimal performance.
\end{itemize}

\subsubsection{Pre-trained Models (SuperPoint)}
Utilizing pre-trained models, like Superpoint, that are trained on large datasets are often highly accurate relative to their efficiency. They are trained on a variety of datasets and aim to generalize well to a variety of tasks.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy due to extensive pre-training on large and diverse datasets. 
    \item \textbf{Disadvantages:} May not generalize well to specific datasets which are not representative of the application dataset.
\end{itemize}



\section{Evaluation for UAV-Based Navigation}
\subsection{Requirements}
For the Skripsie project, the feature extractor must meet the following initial requirements with default parameters:
\begin{itemize}
    \item 
    \item 
    \item Must be free to use and not require any pre-training or live-training. The former excludes SURF, SIFT, and the latter, deep learning-based models. 
\end{itemize}

\subsection{Recommended Feature Extractors}
The feature extractor should be applicable to application specific requirements. In the case of this project, it should be free to use and not require any pre-training or live-training due to scope. As such, the following feature extractors are recommended:
\begin{itemize}
    \item ORB: A fast and efficient feature extractor that provides a good balance between speed and accuracy. It is suitable for real-time applications and can handle scale and rotation changes effectively.
    \item AKAZE: A feature extraction method that combines speed and accuracy by using nonlinear scale space and feature detection. It is robust to various transformations and lighting conditions.
    \item SuperPoint: A pre-trained model that offers high accuracy and efficiency for real-time applications. It learns feature representations directly from data, making it adaptable to specific tasks.


    


\chapter{Results}

TESTS:
amt of keypoints for a specific runtime that are accepted as good matches
overall accuracy for heading and GPS change estimation


DATASET PARAMETERS:
image resolution vs speed accuracy
dataset terrain



All parameter choices

1. feature extractor
2. Feature matcher
3. color or grayscale
4. Blur kernel size
5. percentile cutoff
6. outlier removal method
7. Amt of features to detect
8. Extractor threshold
9. How many matches to detect
10. Match threshold
11. Image size and resolution
12. Scaling factor for GPS to pixels
13. Speed / runtime
14. Accuracy   




--------- AKAZE
AT threshold global : 0.00252

Phase Correlation stability: 1.020 +/- 331.046
Linear Algebra stability: 1.000 +/- 268.356
Affine stability: 1.017 +/- 217.143
Rigid stability: 0.998 +/- 251.669
Homography stability: 1.030 +/- 317.439

Percentage Deviation: [3.28679347] \%
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [44.66974342]
 Mean Heading Error: 0.9414300948366531
Mean Length of Keypoints: 4325.0
Mean Global Time to Extract Keypoints: 0.2766 s
Mean Number of Good Matches: 1805.642857142857
Range of Good Matches: 2544
Time taken to execute The Method: 65.0690 seconds


AKAZE with 0.003252
Phase Correlation stability: 1.021 +/- 328.143
Linear Algebra stability: 1.000 +/- 299.196
Affine stability: 1.027 +/- 183.014
Rigid stability: 1.001 +/- 361.975
Homography stability: 1.043 +/- 362.792

Percentage Deviation: [3.32846695] \%
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [44.0486597]
 Mean Heading Error: 0.8579314885804422
Mean Length of Keypoints: 2896.3333333333335
Mean Global Time to Extract Keypoints: 0.2673 s
Mean Number of Good Matches: 1237.857142857143
Time taken to execute The Method: 48.4716 seconds




AKAZE with 0.00452
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 1.027 +/- 349.645
Linear Algebra stability: 1.000 +/- 305.940
Affine stability: 1.028 +/- 198.647
Rigid stability: 1.001 +/- 289.527
Homography stability: 1.048 +/- 355.014

Percentage Deviation: [3.37979766] \%
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [44.72796619]
 Mean Heading Error: 0.8483008533449834
Mean Length of Keypoints: 1581.7333333333333
Mean Global Time to Extract Keypoints: 0.3385 s
Mean Number of Good Matches: 699.9285714285714
Range of Good Matches: 1003
Time taken to execute The Method: 47.2239 seconds


Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 1.043 +/- 329.260
Linear Algebra stability: 1.000 +/- 365.587
Affine stability: 1.046 +/- 198.964
Rigid stability: 1.001 +/- 636.772
Homography stability: 1.062 +/- 355.714

Percentage Deviation: [3.23767427] \%
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [42.84711685]
 Mean Heading Error: 0.8507801467066372
Mean Length of Keypoints: 686.5333333333333
Mean Global Time to Extract Keypoints: 0.2564 s
Mean Number of Good Matches: 698.0
Range of Good Matches: 522
Time taken to execute The Method: 35.0659 seconds



=====================

ORB: max num kp 20 000
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 0.990 +/- 1182.096
Linear Algebra stability: 1.000 +/- 261.181
Affine stability: 1.023 +/- 195.132
Rigid stability: 0.998 +/- 252.183
Homography stability: 1.034 +/- 310.998

Percentage Deviation: [4.21515911] \%
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [57.28685952]
 Mean Heading Error: 0.9371441256152906 
Mean Global Time to Extract Keypoints: 0.1077 s
Mean Number of Good Matches: 4962.357142857143
Time taken to execute The Method: 396.9090 seconds


ORB: max num kp 10 000 but only take top 2000 (LOC min is 2000 testing in 500s around that)
Phase Correlation stability: 1.007 +/- 358.199
Linear Algebra stability: 1.000 +/- 287.777
Affine stability: 1.008 +/- 140.513
Rigid stability: 0.994 +/- 282.391
Homography stability: 1.023 +/- 343.067

Percentage Deviation: [3.98746533] \%
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [52.7697904]
 Mean Heading Error: 0.8748456988575327
Mean Length of Keypoints: 10000.0
Mean Global Time to Extract Keypoints: 0.0815 s
Mean Number of Good Matches: 2000.0
Time taken to execute The Method: 121.9382 seconds


ORB 20k with 2k top. 
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 0.976 +/- 1171.096
Linear Algebra stability: 1.000 +/- 267.530
Affine stability: 1.008 +/- 161.723
Rigid stability: 0.987 +/- 246.863
Homography stability: 1.016 +/- 321.321

Percentage Deviation: [3.70598935] \%
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [50.36689857]
 Mean Heading Error: 0.9401116673847525
Mean Length of Keypoints: 15000.0
Mean Global Time to Extract Keypoints: 0.0844 s
Mean Number of Good Matches: 2000.0
Time taken to execute The Method: 271.7812 seconds


4k top with top 2000. 
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 1.005 +/- 261.617
Linear Algebra stability: 1.000 +/- 230.847
Affine stability: 1.008 +/- 120.515
Rigid stability: 0.988 +/- 230.963
Homography stability: 1.011 +/- 287.996

Percentage Deviation: [4.02619081] \%
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [53.12582805]
 Mean Heading Error: 0.909769486094396
Mean Length of Keypoints: 4000.0
Mean Global Time to Extract Keypoints: 0.0796 s
Mean Number of Good Matches: 2000.0
Time taken to execute The Method: 52.0125 seconds


On face value, ORB has more accuracy and less time per keypoint extracted, and less total time in the most recent example. However, one also notes how the time varies significantly as the maximum number of keypoints increases. 

AKAZE has a filter threshold which is difficult to tune and highly sensitive to minor changes. Further, this threshold needs to be changed based on the dataset depending on the types of keypoints and visibility etc. As such, it is easier to use the more simple and consistent number of keypoints threshold that ORB uses. To summarize it, ORB is much easier to tune, and tends to have a higher accuracy to time ratio. To achieve the same top local minimum error, AKAZE subtends a runtime of 712 seconds, while ORB subtends a runtime of 271 seconds. This is a significant difference. 

However, AKAZE is significantly more accurate at the same time to run and time per keypoint extracted. It is also more robust to changes in the dataset. Further, a change in time does not subtend a large change in accuracy. However, AKAZE extracts much less, and higher quality keypoints. This is advantageous for efficiency and accuracy, however, it makes AKAZE highly volatile to varying datasets as it will often not acquire sufficient keypoints. This can be mitigated by having an adaptive good match filtering threshold (this is implemented). However, the AKAZE threshold still needs to be manually tuned, this can also be made dynamic, at a time cost for constant re-estimation. 

However, when trying different data sets etc - and perhaps when trying it with the local matcher, robustness might affect these end results. 



