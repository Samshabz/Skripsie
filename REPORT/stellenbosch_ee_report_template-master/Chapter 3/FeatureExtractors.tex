
\subsection*{Feature Extractors}


\subsubsection*{What is a feature Extractor}
A feature extractor is a computer vision algorithm or deep-learning model that identifies and extracts key features in an image. A feature is a pixel or weight of multiple pixels that represents a highly unique and distinctive part of an image. For instance, this could be an edge between a wall and the sky. Feature extractors are characterized by a spatial coordinate and a descriptor. A descriptor is a vector which contains information about the helps it be uniquely identified again in a new image. This can include information about the size, shape, and intensity of the feature. One can also infer the scale and orientation of the feature from the descriptor. A feature extractor also commonly includes a confidence score which indicates a score of how unique and well-defined the feature is. Ultimately, a feature extractors goal is to find unique and well-defined features that can be used to develop a complex understanding of the nuances and details of an image.
\subsubsection*{Usage in this task}
In this task, feature extractors are used to extract keypoints in images. These are used for intra-image rotational and translational estimates, and matching similar images. 



feature extractors are nb bc they r iinvariant to scale rotation and transformation. As opposed to simply correlating entire image. also less computationally expensive.
https://baotramduong.medium.com/feature-extraction-in-computer-vision-using-python-358d7c9863cb



common feature extraction techniques include edge detection, color histograms, and texture analysis



\section{Choosing a Feature Extractor}
When selecting a feature extractor, several critical factors must be considered:
\begin{itemize}
    \item \textbf{Accuracy:} The extractor needs to be accurate to ensure GPS inference is accurate. That is, the extractor should provide a sufficient quality and quantity of keypoints. This is quantified as subtending over 500 good matches from similar image pairs. 
    \item \textbf{Speed:} Be capable of real-time processing to ensure timely navigation and decision-making in dynamic environments. Specifically, initial tests must extract features in less than 1 second on a CPU. For the neural network-based models, the time taken to extract features must be less than 3 second on a CPU since they may be implemented with a GPU and CUDA libraries to improve performance. 
    \item \textbf{Robustness:} The feature extractor should exhibit invariance to changes in scale, rotation, illumination, perspective and noise to ensure accurate and repeatable performance across different flight datasets and conditions.
\end{itemize}

Feature extractors have different parameters trading-off accuracy and speed. For example, there are detection thresholds, descriptor sizes, and keypoint densities that can be adjusted to optimize performance. 

\section{Types of Feature Extractors}
\subsection{Traditional Feature Extractors}
\subsubsection{SIFT (Scale-Invariant Feature Transform)}
SIFT detects and describes local features in images. It is robust to changes in scale, rotation, and illumination, making it a reliable choice for many applications. However, its computational intensity can be a drawback in real-time scenarios.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy and robustness due to its multi-scale approach and precise keypoint localization. SIFT's descriptors are highly distinctive, enabling reliable matching across different views and conditions.
    \item \textbf{Disadvantages:} High computational cost and slower processing speed due to the extensive keypoint detection and descriptor computation steps, making it less suitable for real-time applications.
\end{itemize}


\subsubsection{SURF (Speeded-Up Robust Features)}
SURF is a faster alternative to SIFT, utilizing integral images for rapid computation of image convolutions. It offers good accuracy and robustness while being computationally more efficient than SIFT.
\begin{itemize}
    \item \textbf{Advantages:} Faster than SIFT due to its use of Haar wavelets and integral images, providing good balance between speed and accuracy. It maintains robustness to scale and rotation changes.
    \item \textbf{Disadvantages:} Still relatively computationally expensive compared to simpler methods like ORB, and can be less accurate than SIFT in certain complex scenarios.
\end{itemize}



\subsubsection{ORB (Oriented FAST and Rotated BRIEF)}
ORB combines the FAST keypoint detector and the BRIEF descriptor, providing a highly efficient feature extraction method suitable for real-time applications. It is designed to be both fast and invariant to rotation and scale.
\begin{itemize}
    \item \textbf{Advantages:} High speed and efficiency, making it suitable for real-time applications. ORB's binary descriptors are computationally less intensive while providing sufficient discriminative power for many tasks.
    \item \textbf{Disadvantages:} Lower accuracy compared to SIFT and SURF, especially in complex scenes with significant variations in lighting and scale. The binary nature of BRIEF descriptors can sometimes lead to higher false match rates.
\end{itemize}

\subsection{AKAZE (Accelerated-Keypoint-Affine-Zernike)}
AKAZE is a feature extraction method that combines speed and accuracy by using nonlinear scale space and feature detection. It is designed to be robust to various transformations and lighting conditions.
\begin{itemize}
    \item \textbf{Advantages:} High speed and efficiency due to its nonlinear scale space and feature detection approach. AKAZE is robust to scale, rotation, and illumination changes, making it suitable for diverse environments.
    \item \textbf{Disadvantages:} May not be as accurate as SIFT or SURF in certain scenarios, particularly those with complex textures or repetitive patterns. The trade-off between speed and accuracy may vary depending on the application.
\end{itemize}

\subsection{Deep Learning-Based Feature Extractors}
Deep learning-based feature extractors leverage neural networks to learn feature representations directly from data. These models need to be trained on large datasets to capture complex and hierarchical features effectively. They offer high accuracy and the ability to adapt to specific tasks through transfer learning.
    \item \textbf{Advantages:} High accuracy and the ability to learn complex and hierarchical features directly from data, enabling robust performance across diverse tasks and conditions. 
    \item \textbf{Disadvantages:} Requires substantial computational resources for training and inference. The training process is data-intensive, requiring large labeled datasets to achieve optimal performance.
\end{itemize}

\subsubsection{Pre-trained Models (SuperPoint)}
Utilizing pre-trained models, like Superpoint, that are trained on large datasets are often highly accurate relative to their efficiency. They are trained on a variety of datasets and aim to generalize well to a variety of tasks.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy due to extensive pre-training on large and diverse datasets. 
    \item \textbf{Disadvantages:} May not generalize well to specific datasets which are not representative of the application dataset.
\end{itemize}



\section{Evaluation for UAV-Based Navigation}
\subsection{Requirements}
For the Skripsie project, the feature extractor must meet the following initial requirements with default parameters:
\begin{itemize}
    \item 
    \item 
    \item Must be free to use and not require any pre-training or live-training. The former excludes SURF, SIFT, and the latter, deep learning-based models. 
\end{itemize}

\subsection{Recommended Feature Extractors}
The feature extractor should be applicable to application specific requirements. In the case of this project, it should be free to use and not require any pre-training or live-training due to scope. As such, the following feature extractors are recommended:
\begin{itemize}
    \item ORB: A fast and efficient feature extractor that provides a good balance between speed and accuracy. It is suitable for real-time applications and can handle scale and rotation changes effectively.
    \item AKAZE: A feature extraction method that combines speed and accuracy by using nonlinear scale space and feature detection. It is robust to various transformations and lighting conditions.
    \item SuperPoint: A pre-trained model that offers high accuracy and efficiency for real-time applications. It learns feature representations directly from data, making it adaptable to specific tasks.


    


\chapter{Results}

TESTS:
amt of keypoints for a specific runtime that are accepted as good matches
overall accuracy for heading and GPS change estimation


DATASET PARAMETERS:
image resolution vs speed accuracy
dataset terrain



All parameter choices




test 1: accuracy on a single dataset for rotational estimation (global matcher), global matching technique, and local matcher. gonna need to split those. say something about the matches are good no matter what  - not a hard task.
So test both methods (Plus neural local) on both stages (2x3 results) on a single dataset - see which is more accurate only. - total time and acc.
test 2: Take the top 3 combinations and test them on all the datasets (or as many as are close together in accuracy) - optimize for datasets - see which is more accurate only. - total time and acc. 
test 3: mess up the parameters and see how it affects the stability of different methods as well as the overall accuracy - stability, acc and time. 
test 4: test whether one performs significantly better than normally when used as the global matcher technique

XXX - note that we dont look at time per extraction as the time of latter stages is affected by the amount and quality of keypoints. So it might extract rubbish faster, but then take longer to match. So we look at total time.
Test 1 and 2 show accuracy. Test 2 and 3 show robustness. 

XXX - need to make a note on why mean heading error is not compared much - it subtends GPS error - and its estimated in this project as we dont have an accurate heading. 

XXX - say we are going to use histograms, its significantly faster, ensures no effects from the global match, could potentially test that after 







On face value, ORB has more accuracy and less time per keypoint extracted, and less total time in the most recent example. However, one also notes how the time varies significantly as the maximum number of keypoints increases. 

AKAZE has a filter threshold which is difficult to tune and highly sensitive to minor changes. Further, this threshold needs to be changed based on the dataset depending on the types of keypoints and visibility etc. As such, it is easier to use the more simple and consistent number of keypoints threshold that ORB uses. To summarize it, ORB is much easier to tune, and tends to have a higher accuracy to time ratio. To achieve the same top local minimum error, AKAZE subtends a runtime of 712 seconds, while ORB subtends a runtime of 271 seconds. This is a significant difference. 

However, AKAZE is significantly more accurate at the same time to run and time per keypoint extracted. It is also more robust to changes in the dataset. Further, a change in time does not subtend a large change in accuracy. However, AKAZE extracts much less, and higher quality keypoints. This is advantageous for efficiency and accuracy, however, it makes AKAZE highly volatile to varying datasets as it will often not acquire sufficient keypoints. This can be mitigated by having an adaptive good match filtering threshold (this is implemented). However, the AKAZE threshold still needs to be manually tuned, this can also be made dynamic, at a time cost for constant re-estimation. 

However, when trying different data sets etc - and perhaps when trying it with the local matcher, robustness might affect these end results. 





NOTE ORB simply does not work on the desert dataset 





Please note that these tests are run without optimal settings - i.e. for debug purposes certain parts run which normally would not. 




TEST 1:
Heading estimation error too high.
Heading estimation error too high.
Linear regression inferred factor x: 3.207310914993286
Linear regression inferred factor y: 3.526571035385132 

Heading estimation error too high.
Heading estimation error too high.
Heading estimation error too high.
Heading estimation error too high.
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 1.159 +/- 68807.336
Linear Algebra stability: 1.000 +/- 56.929
Affine stability: 1.020 +/- 921.300
Rigid stability: 0.995 +/- 160.067
Homography stability: 1.108 +/- 5592.924

Percentage Deviation: [3.22100261] %
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: ORB, Local Matcher: BF
Mean normalized GPS error: [24.86177763]
 Mean Heading Error: 1.9734131972502695
Mean Length of Global Keypoints: 5426.8
Mean Length of Local Keypoints: 5426.8
Mean Global Time to Extract Keypoints: 0.0295 s
Mean Number of Good Matches: 604.5387931034483
Range of Good Matches: 7310
Time taken to execute The Method: 57.5468 seconds



END 1

Heading estimation error too high.
Heading estimation error too high.
Linear regression inferred factor x: 3.2284045219421387
Linear regression inferred factor y: 3.480206251144409

Heading estimation error too high.
Heading estimation error too high.
Heading estimation error too high.
Heading estimation error too high.
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 1.163 +/- 68807.336
Linear Algebra stability: 1.000 +/- 43.544
Affine stability: 1.026 +/- 595.532
Rigid stability: 0.994 +/- 195.841
Homography stability: 1.111 +/- 5592.908

Percentage Deviation: [3.11905048] %
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [24.07484528]
 Mean Heading Error: 1.9734131972502695
Mean Length of Global Keypoints: 5426.8
Mean Length of Local Keypoints: 9987.666666666666
Mean Global Time to Extract Keypoints: 0.0337 s
Mean Number of Good Matches: 604.5387931034483
Range of Good Matches: 7310
Time taken to execute The Method: 67.8058 seconds


END 2

Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 1.190 +/- 68807.336
Linear Algebra stability: 1.000 +/- 176.334
Affine stability: 1.101 +/- 2279.378
Rigid stability: 0.989 +/- 473.279
Homography stability: 1.141 +/- 5715.823

Percentage Deviation: [2.75680053] %
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: SUPERPOINT, Local Matcher: LightGlue
Mean normalized GPS error: [21.27876627]
 Mean Heading Error: 1.9734131972502695
Type: Numpy Array
Shape: torch.Size([1, 1000, 2])
Mean Length of Global Keypoints: 5426.8
Mean Length of Local Keypoints: 1000.0
Range of Local kp: 0
Mean Global Time to Extract Keypoints: 0.0342 s
Mean Number of Glob good Matches: 604.5387931034483
Time taken to execute The Method: 130.9345 seconds

Note that superpoint and Lightglue have various tunable parameters. However, there is no way to reduce the time taken to extract and match keypoints to the point of the algorithmic models without significantly reducing accuracy. However, these models are significantly faster on GPU's and CUDA libraries. There is a decent range where one can increase time cost and gain a reasonable increase in accuracy. Superpoint and LightGLue parameters are relatively hard to tune, as there are many. However, XXX, its yet to be seen how robust these parameters are to different datasets. 

END 3


Heading estimation error too high.
Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 0.877 +/- 70819.259
Linear Algebra stability: 1.000 +/- 62.694
Affine stability: 0.998 +/- 1355.790
Rigid stability: 0.987 +/- 181.514
Homography stability: 1.044 +/- 6260.942

Percentage Deviation: [3.05771815] %
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: ORB, Local Matcher: BF
Mean normalized GPS error: [20.64342049]
 Mean Heading Error: 1.7089258086216432
Mean Length of Global Keypoints: 55.53333333333333
Mean Length of Local Keypoints: 5426.8
Range of Local kp: 7310
Mean Global Time to Extract Keypoints: 0.2273 s
Mean Number of Glob good Matches: 300.0
Time taken to execute The Method: 26.1905 seconds


END 4


Stability analysis mean (relative to src - best result) and var*10e6
Phase Correlation stability: 0.905 +/- 70819.259
Linear Algebra stability: 1.000 +/- 166.490
Affine stability: 1.044 +/- 2069.720
Rigid stability: 0.983 +/- 324.556
Homography stability: 1.077 +/- 6339.018

Percentage Deviation: [2.77759658] %
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: SUPERPOINT, Local Matcher: LightGlue
Mean normalized GPS error: [18.75224969]
 Mean Heading Error: 1.7089258086216432
Mean Length of Global Keypoints: 55.53333333333333
Mean Length of Local Keypoints: 1000.0
Range of Local kp: 0
Mean Global Time to Extract Keypoints: 0.2408 s
Mean Number of Glob good Matches: 300.0
Time taken to execute The Method: 77.1795 seconds

END 5








Another note, this data is run in debug mode, with many methods running sequentially instead of a single method. Further, this is ran in a high accuracy mode for both, with an aim to get good results for both while keeping the time between methods reasonably similar without moving far off optimal points. 








ADDITIONAL TEST:
Percentage Deviation: [2.65627788] %
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [18.24802137]
 Mean Heading Error: 1.545597672500035
Mean Length of Global Keypoints: 4083.133333333333
Mean Length of Local Keypoints: 15055.266666666666
Range of Local kp: 12695
Mean Global Time to Extract Keypoints: 0.2440 s
Mean Number of Glob good Matches: 300.0
Time taken to execute The Method: 113.2767 seconds
with akaze 0.00017, and 0.00007 for local. 



akaze akaze:
Phase Correlation stability: 0.902 +/- 46811.385
Linear Algebra stability: 1.000 +/- 38.748
Affine stability: 1.023 +/- 805.875
Rigid stability: 0.986 +/- 202.970
Homography stability: 1.114 +/- 5841.347

Percentage Deviation: [2.80564334] %
Preprocessing Global Detector: AKAZE, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: AKAZE, Local Matcher: BF
Mean normalized GPS error: [19.27412787]
 Mean Heading Error: 1.6004082453122863
Mean Length of Global Keypoints: 1296.9333333333334
Mean Length of Local Keypoints: 15055.266666666666
Range of Local kp: 12695
Mean Global Time to Extract Keypoints: 0.2092 s
Mean Number of Glob good Matches: 300.0
Time taken to execute The Method: 56.3528 seconds


akaze orb (local):
XXX make note abt dynamic adjustment 
