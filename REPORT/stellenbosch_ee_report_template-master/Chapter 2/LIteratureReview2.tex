
\chapter{Literature Review}
\vspace{-1cm}



\section{Related Work}

Autonomous navigation for Unmanned Aerial Vehicles (UAVs) has been extensively studied, with prominent methods like Simultaneous Localization and Mapping (SLAM) providing comprehensive mapping and localization capabilities. SLAM enables UAVs to construct detailed maps of their environment while simultaneously tracking their position within these maps \cite{arafat2023vision}. Typically, SLAM systems integrate data from multiple sensors—such as cameras, LiDAR, and Inertial Measurement Units (IMUs)—to achieve accurate localization, making them particularly effective in structured, indoor environments like warehouses. However, building and maintaining high-resolution maps in SLAM is computationally intensive, requiring significant processing power and memory \cite{arafat2023vision}. This computational burden poses challenges for real-time applications in resource-limited UAVs. Additionally, SLAM systems are sensitive to map distortions and inaccuracies, which can degrade localization reliability \cite{arafat2023vision}. Moreover, in the context of UAV navigation for return flights, constructing an entire map of the outbound path may be unnecessary, making SLAM an excessive solution in terms of memory consumption and computational resources for such applications.

Optical flow is another technique for motion estimation, which calculates the apparent motion of brightness patterns in the image plane between consecutive frames \cite{sim2002integrated}. Optical flow relies on the brightness constancy assumption and small, smooth movements, which can be violated in cases of abrupt UAV movements, rotations, or scale changes, leading to unreliable motion estimates. Consequently, optical flow is generally more suitable for short-term odometry rather than long-distance reverse path tracking. Additionally, computing dense optical flow across entire frames is computationally intensive, rendering it less practical for real-time applications on resource-constrained UAVs \cite{barnum2003practical}.



Several studies have explored image-based solutions for UAV navigation, utilizing feature matching techniques to estimate the UAV's location.

For instance, Zhang et al. \cite{Zhang2024} introduced LoFTRS, a deep learning-based image matching method with semantic constraints to improve matching accuracy. LoFTRS provides refined feature correspondences that strengthen subsequent UAV localization tasks. While this study offers valuable insights into potential pipelines and highlights the benefits of efficient feature extractors, it lacks a detailed implementation, comparative analysis of methods, and practical testing across diverse environments.

Another study by Sim et al. \cite{sim2002integrated} proposed an image-based location estimation approach that combines relative positioning techniques through aerial image sequences to improve short-term odometry, enhancing UAV navigation reliability over short distances. Their method emphasizes the importance of constant global correction by normalizing to a global reference frame. However, their approach does not consider the use of fixed reference images for navigation, which limits its applicability for reference-image-based navigation in GNSS-denied environments.


However, these existing methods do not provide detailed descriptions of the complete navigation pipeline, including the selection of specific methods and parameter choices. Moreover, they do not assess the system's ability to generalize across diverse environments or test its robustness under typical operational challenges faced during UAV missions.

To address these gaps, this study presents a comprehensive image-based navigation pipeline for UAVs. It includes in-depth comparisons of different methods, emphasizes environmental adaptability, and undergoes rigorous testing under adverse conditions. This approach aims to provide a reliable and scalable framework for addressing real-world UAV navigation challenges in GNSS-denied environments.


% -----------------------------------------------------------------------------------------------------------------------------------

\section{Fundamental Concepts and Techniques}

This section provides a comprehensive overview of the fundamental concepts and techniques that underpin the proposed image-based UAV navigation system. By delving into feature extraction, matching, and planar transformations, it establishes the theoretical foundation essential for the subsequent system design and implementation. The discussion emphasizes the critical role of feature-based methods over direct approaches, setting the stage for understanding the chosen methodologies.


% -----------------------------------------------------------------------------------------------------------------------------------



\subsection{Feature Detectors}

Feature extraction is a cornerstone of image-based UAV navigation, enabling the estimation of transformations such as rotation and translation between consecutive images. Feature detectors identify \textbf{keypoints}—distinct, repeatable points within an image—and generate \textbf{descriptors} that encapsulate information about the local image region surrounding each keypoint. These keypoints and descriptors, or simply features, are essential for accurate matching across multiple frames, allowing the system to track movement while maintaining invariance to changes in scale, rotation, and illumination. An example of feature detection and matching, the latter explained in the subsequent section, is illustrated in Figure ~\ref{fig:Matches_CITY2}. This image shows the top 50 matches between two images after rotational alignment. 




\subsubsection{ORB (Oriented FAST and Rotated BRIEF)}

\textbf{ORB} combines the \textbf{FAST} \cite{trajkovic1998fast} keypoint detector with the \textbf{BRIEF} \cite{calonder2010brief} descriptor, enhanced for rotation invariance. \textbf{FAST} rapidly identifies keypoints by analyzing pixel intensity differences in a circular region around each candidate point. Once detected, \textbf{BRIEF} encodes the local image patch into a binary string through intensity comparisons. ORB introduces rotational invariance by aligning keypoints based on their dominant orientation before descriptor computation. This enhancement makes ORB both extremely fast and robust to scale and in-plane rotation, although it may struggle with repetitive textures or complex lighting variations \cite{tareen2018comparative}. 

\subsubsection{AKAZE (Accelerated-KAZE)}

\textbf{AKAZE} constructs a nonlinear scale space using diffusion-based filtering, capturing finer image details more effectively than linear methods. It detects keypoints by assessing local contrast with a specialized adaptive filter, enabling the identification of subtle features that simpler detectors might miss. The \textbf{Modified Local Difference Binary (MLDB)} \cite{reddy2021implementation} descriptor encodes the neighborhood of each keypoint into a binary vector based on pixel intensity differences. While AKAZE is both fast and compact, its performance can be sensitive to detection thresholds across different environments, potentially affecting its robustness in varied operational contexts \cite{opencv_akaze}. However, a notable strength of AKAZE is its rotational invariance \cite{tareen2018comparative}.

\subsubsection{SuperPoint with LightGlue}
\label{sec:SuperPoint with LightGlue}
\textbf{SuperPoint} is a deep learning-based keypoint detector and descriptor that leverages convolutional neural networks (CNNs) to identify and describe keypoints in a single forward pass. Pre-trained on extensive image datasets, SuperPoint excels at recognizing stable and distinctive keypoints under varied conditions \cite{rpaultrat2023superpoint}. However, its performance may degrade on datasets significantly different from its training data. Pairing SuperPoint with \textbf{LightGlue}, a machine-learning-based matcher, enhances matching accuracy through advanced graph-based techniques that were recognized at the 2023 International Conference on Computer Vision \cite{cvg2023lightglue}. Despite their high accuracy, SuperPoint and LightGlue are computationally intensive and require GPU acceleration for real-time applications. However, their improved performance justifies their inclusion in this study, even though they may not achieve real-time performance when tested on a CPU alone \cite{rpaultrat2023superpoint}.


% -----------------------------------------------------------------------------------------------------------------------------------


\subsection{Feature Matching}

Feature matching establishes correspondences between keypoints in different images based on descriptor similarity. After identifying these correspondences, ambiguities and low-quality matches are removed, as detailed in Section ~\ref{sec:Optimization Techniques}. 

Each matcher generates a list of potential matches along with their similarity scores, quantified using a descriptor-space distance metric. These scores are instrumental in subsequent filtering processes.

Feature matching involves two primary components: the choice of matching technique to acquire potential matches and the search technique that determines which of these matches to retain. 

\subsubsection{Types of Feature Matching Techniques}

The following match acquisition techniques are commonly employed in feature-based navigation systems:

\textbf{Brute-Force Matcher (BFMatcher)}: The Brute-Force Matcher is a simple, exhaustive matcher that matches each feature in one image with every feature in the second image, ensuring the best possible match based on descriptor similarity. While this guarantees high accuracy, it is computationally expensive, especially with large numbers of keypoints, making it less suitable for real-time applications without optimization \cite{opencv_bfmatcher}.

\textbf{Fast Library for Approximate Nearest Neighbours (FLANN)}: FLANN accelerates the nearest neighbour search in high-dimensional descriptor spaces using algorithms such as KD-trees or hierarchical clustering, adapting dynamically to the dataset. This approximate matching approach offers significant speed improvements with minimal loss in accuracy, making it ideal for real-time applications with extensive datasets \cite{muja2014scalable}.

\textbf{LightGlue}: Explained in Section ~\ref{sec:SuperPoint with LightGlue}.


The following search techniques are commonly used to filter matches and retain only the most reliable correspondences:

\textbf{Radius Search}: This method retains matches within a specified distance in descriptor space, effectively filtering out weaker matches. However, it does not guarantee a fixed number of matches per keypoint, leading to inconsistent results \cite{opencv_matcher_tutorial}.

\textbf{K-Nearest Neighbours (KNN) Matching}: KNN matching retains the top K matches for each keypoint, allowing the application of post-filtering techniques such as Lowe’s ratio test to eliminate ambiguous matches \cite{opencv_matcher_tutorial}.

\textbf{Vanilla Matching}: Vanilla matching returns the single best match for each keypoint based on the closest descriptor distance. It is a subset of KNN matching with K=1, offering simplicity and ease of implementation \cite{opencv_matcher_tutorial}.




% ------------------------------------------------------------------------------------------------------------------------

\subsection{Image Similarity Computation}

Image similarity computation is a pivotal component of UAV navigation systems that rely on reference images for accurate localization and pose estimation. Effective similarity measures ensure efficient processing of extensive image datasets and facilitate precise transformation estimations, which are essential for reliable navigation.

\subsubsection{2.2.3.1 Proximity-Based Techniques}

To achieve efficient, real-time performance, the search space is reduced to images within the proximity of UAV's last known location, filtering images within a static or dynamic radius. While this method is highly efficient, it does not account for potential deviations from the expected flight path or the presence of poor-quality reference images. This limitation implies that this measure cannot be used as the sole basis for image similarity computation, necessitating the integration of additional techniques for comprehensive assessment.


\subsubsection{2.2.3.2 Global Matching Techniques}

To ensure images are evaluated for similarity and ensure they are free from significant distortion, global matching, or direct methods are employed. Direct methods estimate planar transformations by comparing entire image pixel intensities and minimizing differences through optimization techniques like gradient descent. Because they consider the entire image context, these methods are well-suited for similarity comparisons; however, they are not ideal for precise transformation estimation due to their sensitivity to noise and illumination changes \cite{GlobalLocal2023}. The following methods are global matching techniques commonly used in image similarity computation:


\textbf{Cross-Correlation}

Cross-correlation measures similarity by sliding one image over another and computing the sum of pixel-wise multiplications at each position. The peak value signifies the best alignment, and its magnitude indicates the confidence level of the similarity. Higher confidence values reflect greater similarity between the images. While straightforward to implement, cross-correlation is sensitive to noise and illumination changes, which can compromise the reliability of the similarity measure \cite{sharma2022crosscorrelation}.

\textbf{Histograms}

Histogram comparison assesses similarity by analyzing the distribution of pixel intensities within each image. Typically, each image's histogram is divided into 256 intensity bins for 8-bit images, and similarity is quantified using metrics such as Chi-Square or Bhattacharyya distance. This method emphasizes global color and brightness distributions but neglects spatial information, making it less effective for nuanced structural differences \cite{rosebrock2014comparehistograms}.

\textbf{Structural Similarity Index (SSIM)}

SSIM evaluates similarity by decomposing images into luminance, contrast, and structure components. It computes local statistics within small windows and integrates them into a single similarity score that mirrors perceived image quality. SSIM effectively captures structural information like edges and textures, aligning closely with human visual perception. Although slightly more computationally expensive than the former methods, it is robust to varied conditions \cite{rosebrock2017imagedifference}. 


\textbf{Local Detectors Conversion}
Although not inherently a global matching technique, local feature matching can be adapted to achieve a global understanding of image similarity. This involves identifying and matching keypoints in both images and assessing the overall number of good matches. However, this approach alone does not ensure an even distribution of matches across the entire image, potentially leading to biased, localized similarity assessments. To mitigate this, a grid matching technique is employed, dividing the image into grids and limiting the number of matches per grid. Although the most computationally intensive, this method enhances robustness against distortions and rotations by ensuring a uniform distribution of matches across the image. To maintain reasonable runtime, this method has to employ a very crude detection and matching layer. 



% -----------------------------------------------------------------------------------------------------------------------------------


\subsection{Planar Transformation Estimators}

Feature-based methods extract and match keypoints from both reference and real-time images, often incorporating outlier removal stages \cite{GlobalLocal2023}. By focusing on distinctive features rather than every pixel, these methods excel in handling large viewpoint changes and rotations, enabling more precise transformation inference. This targeted approach enhances computational efficiency and robustness to environmental distortions, though it requires careful management to avoid performance degradation from variation in the number of extracted feature points. 

In typical UAV flight scenarios, the primary transformations of interest are rotation and translation. Perspective distortion, caused by three-dimensional structures, is minimal at high altitudes. Similarly, shear distortion, which occurs when the UAV turns or is not parallel to the ground, is negligible when the UAV maintains level flight. Furthermore, scaling is not present as per the scope of this study. The following subsections detail the primary planar transformations employed in the system.

\subsubsection{Affine Transformation}

Affine transformation captures translation, rotation, scaling, and shear, providing six degrees of freedom. It is represented by a  \(2 \times 3\) matrix that maps points from one plane to another while preserving lines and parallelism. Affine transformations are computed by estimating the affine transformation matrix between two sets of corresponding points using OpenCV's \texttt{estimateAffine2D} function \cite{opencv_warp_affine}. While versatile, the inclusion of scaling and shear introduces unnecessary error points in the UAV case.

\subsubsection{Rigid Transformation Estimation (SVD)}

The rigid transformation via SVD preserves the shape and size of objects by estimating only rotation and translation, excluding scaling and shear. Represented by a  \(2 \times 3\) matrix, rigid transformation ensures orthogonality in the rotation component. Utilizing Singular Value Decomposition (SVD), this method minimizes the least-squares error between two point sets. The process involves: computing the weighted centroids of both point sets, centering the points by subtracting their respective centroids, calculating the covariance matrix of the centered points, performing SVD on the covariance matrix to derive the rotation matrix, and determining the translation vector based on the centroids. The resulting \(2 \times 2\) rotation matrix and \(2 \times 1\) translation vector are combined to form the rigid transformation matrix. Rigid transformation is computationally efficient and well-suited for UAV applications \cite{sorkine2017least_squares}.


\subsubsection{Partial Affine Transformation}

Partial affine transformation simplifies the full affine model by focusing solely on translation, rotation, and limited uniform scaling, offering four degrees of freedom. This transformation is also represented by a \(2 \times 3\) matrix, similar to the affine transformation but without shearing and with reduced, uniform scaling. This offers similar performance to the prior rigid method, but with minor additional error points due to scaling \cite{opencv_warp_affine}.

\subsubsection{Homography Transformation}

Homography transformation accounts for translation, rotation, scaling, shear, and perspective distortion, providing eight degrees of freedom. It is represented by a \(3 \times 3\) matrix and is estimated using OpenCV's \texttt{findHomography} function, typically with RANSAC for outlier rejection \cite{opencv_homography}. While homography offers greater flexibility in modeling complex, typically three-dimensional transformations, its additional degrees of freedom introduce unnecessary errors and computational overhead for UAV-based applications.



% -----------------------------------------------------------------------------------------------------------------------------------
\subsection{Optimization Techniques}
\label{sec:Optimization Techniques}

Optimization techniques aim to refine the accuracy and reliability of the matched points used for transformation estimation by effectively filtering out erroneous matches and improving transformation accuracy. The following subsections detail the primary optimization methods employed in this system.

\subsubsection{Random Sample Consensus (RANSAC) for Planar Transformation}

RANSAC is a robust estimation technique used to estimate planar transformations by iteratively selecting random subsets of point correspondences to fit a model and identify inliers \cite{fisher2002ransac}. The process involves randomly selecting a minimal subset of point pairs, estimating the transformation model (e.g., affine or homography) based on the selected subset, determining the number of inliers that fit the estimated model within a predefined threshold, and repeating the process for a set number of iterations or until a sufficient inlier ratio is achieved. This approach is highly effective in datasets with significant outliers, focusing on finding a model that best fits the largest subset of inliers. However, due to its iterative nature and the need to sample repeatedly, RANSAC can result in increased runtime, particularly in larger datasets or when dealing with numerous outliers \cite{fisher2002ransac}.

\subsubsection{Least Median of Squares (LMeds) for Planar Transformation}

Least Median of Squares (LMeds) is a robust estimation technique used for estimating planar transformations by minimizing the median of the squared residuals between matched points \cite{farin2005video}. Unlike RANSAC, which focuses on maximizing the number of inliers, LMedS aims to find a model that minimizes the median error, making it less sensitive to outliers. The process involves calculating the transformation parameters that result in the lowest median of squared residuals across all data points. While LMedS can be more robust in the presence of outliers compared to RANSAC, it can be computationally more intensive and may not perform as well when the percentage of outliers is high.


\subsubsection{Lowe's Ratio Test}

Lowe's ratio test is a widely used filtering technique used to eliminate ambiguous or false keypoint matches by comparing the distance of the best match to the second-best match \cite{bian2020gms}. For each keypoint match, the ratio of the distance of the best match to that of the second-best match is calculated, and the match is retained if this ratio is below a predefined threshold. A lower ratio indicates that the best match is significantly better than the alternatives, thereby increasing the likelihood of the match being correct. An example of Lowe's ratio test filtration is shown in Figure~\ref{fig:lowes1} and Figure~\ref{fig:lowes2}.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.37\textwidth}
        \includegraphics[width=\textwidth]{./Chapter 2/litfigs/lowes1.png}
        \caption{Matching Features Without Lowe's Ratio Test.}
        \label{fig:lowes1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.37\textwidth}
        \includegraphics[width=\textwidth]{./Chapter 2/litfigs/lowes2.png}
        \caption{Matching Features With Lowe's Ratio Test.}
        \label{fig:lowes2}
    \end{subfigure}
    \caption{Reproduced from \cite{bian2020gms}.}
    \label{fig:lowes}
\end{figure}


\subsubsection{N-Match or Absolute Thresholding}
N-match thresholding involves setting a threshold that allows only a specific number of matches with the smallest descriptor distances to be retained. Absolute thresholding filters matches based on a fixed distance in descriptor space. Only matches that meet or fall below this predefined distance threshold are retained, ensuring that only sufficiently similar matches are used in the transformation estimation process. These methods primarily suffer from difficulty in setting the threshold, which is often extremely sensitive to dataset variations and method parameters.
