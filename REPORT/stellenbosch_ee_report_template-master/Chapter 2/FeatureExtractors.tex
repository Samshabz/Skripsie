\chapter{Feature Extractors}


2. What information did u draw from or acquire to solve problem. What did I learn not from EE engineering. Additional to undergrad. 

e.g. water quality: UV spectrum, optical sensing, color spectrum moving signals. Capture additional technologies and information to do design. Anything beyond scope of EE, lecturer might not know. I needed to know this before addressing problem. 

eg i have these 3 options at my disposal or many. 

\section{Intropara}

This chapter focuses on the additional prerequisite knowledge required to solve the problem above what the reader is expected to know. The image-based navigation system involves many functional parts which need to be understood and integrated to solve the problem. This includes the technique of feature extraction, feature matching, and homographic estimation based on the change in rotation and translation of the landscape. 

\subsection*{Feature Extractors}
In this report, we conduct an in-depth evaluation of various feature extractors to determine the most suitable one for real-time, robust and accurate UAV-based navigation. 


- what is a feature extractor   
- why is it important
- what are the factors that affect the performance of a feature extractor 
- what are the types of feature extractors (How many are there)
- Parameters of feature extractors
- which ones solve the problem. e.g. built for identifying people is not good for landscapes. 
- which are the best ones for our problem
- what are the parameters associated with feature extractor optimization 
- techniques to optimize feature extractors (plots, optimization techniques etc)
- how to evaluate feature extractors
- what are the limitations of feature extractors


\subsection{Feature Matchers}

- What is a feature matcher
- Why is it important
- What are the types of feature matchers (How many are there)
- Which ones solve the problem. e.g. good at matching people is not good for landscapes.
- What are the factors that affect the performance of a feature matcher
- Which are the best ones for our problem
- What are the parameters associated with feature matcher optimization
- Techniques to optimize feature matchers (plots, optimization techniques etc)
- How to evaluate feature matchers
- What are the limitations of feature matchers


\subsection{Homographic Estimation} 
- What is homographic estimation (rot translation warping)
- Why is it important
- What are the types of homographic estimation (How many are there)
- what are the alternatives to homographic estimation (e.g. optical flow, affine transformation, and perspective transformation)
- What is the most accurate way to estimate homography
- What are the factors that affect the performance of homographic estimation    
- How should we optimize homographic estimation
- How do we evaluate the performance of homographic estimation
- What are the limitations of homographic estimation

\subsection*{Accuracy considerations}
- Pixel to GPS coordinate conversion factor accuracy
- Warping in the data (fisheye lens, distortion)
- Image resolution and quality
- Depth of objects causing relatively velocity differences
- Parameters of extraction and matching
- Preprocessing of images (e.g., blurring)
- Outlier removal techniques

\subsection*{Stereo Vision}
- What is stereo vision
- Why is it important
- What are the techniques of stereo vision (How many are there)
- What are the factors that affect the performance of stereo vision
- What is the basic data requirements for accurately estimating depth
- How should we optimize stereo vision
- How can we evaluate the performance of stereo vision
- What are the limitations of stereo vision
- What are the performance considerations of stereo vision




\subsection*{Feature Extractors}
In this report, we conduct an in-depth evaluation of various feature extractors to determine the most suitable one for real-time, robust and accurate UAV-based navigation. 


- what is a feature extractor   
- Usage in this task
- what are the types of feature extractors (How many are there)
- which ones solve the problem. e.g. built for identifying people is not good for landscapes. 
- what are the factors that affect the performance of a feature extractor 
- which are the best ones for our problem
- what are the parameters associated with feature extractor optimization 
- techniques to optimize feature extractors (plots, optimization techniques etc)
- how to evaluate feature extractors
- what are the limitations of feature extractors


\subsubsection*{What is a feature Extractor}
A feature extractor is a computer vision algorithm or deep-learning model that identifies and extracts key features in an image. A feature is a pixel or weight of multiple pixels that represents a highly unique and distinctive part of an image. For instance, this could be an edge between a wall and the sky. Feature extractors are characterized by a spatial coordinate and a descriptor. A descriptor is a vector which contains information about the helps it be uniquely identified again in a new image. This can include information about the size, shape, and intensity of the feature. One can also infer the scale and orientation of the feature from the descriptor. A feature extractor also commonly includes a confidence score which indicates a score of how unique and well-defined the feature is. Ultimately, a feature extractors goal is to find unique and well-defined features that can be used to match images, estimate motion or depth, classify objects and perform other computer vision tasks.

\subsubsection*{Usage in this task}
In this task, feature extractors are used to identify and extract key features in images captured by a UAV. We then use the extracted featured to match images, and ultimately estimate the new GPS coordinates and heading of the UAV. Accurate features are required to find the image that has the most correlation with the current image. Then, many features are required to accurately estimate the new GPS coordinates and heading of the UAV. In both cases, there is a requirement on the extractor to propose as many accurate keypoints as possible to the next stage. 

\subsubsection*{What factors affect the performance of a feature extractor}
The performance of a feature extractor is influenced by several factors, including:
- Accuracy: The ability of the feature extractor to detect and describe features accurately is crucial for reliable image matching and estimation tasks. That is, it should be able to identify unique features that have very distinct and strong descriptors.
- Robustness: The feature extractor should be invariant to changes in scale, rotation, illumination, and noise to ensure consistent performance across different environments and conditions.
- Speed: Real-time processing requires fast computation and feature extraction to minimize latency, which is essential for timely decision-making in dynamic environments.
- Repeatability: The feature extractor should be able to detect the same feature in different images, even if the feature is viewed from different angles or under different lighting conditions.
- Scalability: The algorithm should efficiently handle large datasets or high-resolution images, maintaining performance as the scale of data increases.

\subsubsection*{How many feature extractors exist and what are the main ones?}

There exists a wide variety of feature extractors, each with its unique strengths and weaknesses. Some of the most commonly used feature extractors include: 
- SIFT (Scale-Invariant Feature Transform): SIFT is a classic feature extractor known for its robustness to scale, rotation, and illumination changes. It is widely used in various computer vision applications.
- SURF (Speeded-Up Robust Features): SURF is a faster alternative to SIFT, offering similar robustness and accuracy while being computationally more efficient.
- ORB (Oriented FAST and Rotated BRIEF): ORB is a fast and efficient feature extractor that combines the FAST keypoint detector with the BRIEF descriptor. It is suitable for real-time applications.
- SuperPoint: SuperPoint is a recently developed, deep learning-based feature extractor that offers high accuracy and efficiency for real-time applications. It learns feature representations directly from data, making it adaptable to specific tasks.
- 



feature extractors are nb bc they r iinvariant to scale rotation and transformation. As opposed to simply correlating entire image. also less computationally expensive.
https://baotramduong.medium.com/feature-extraction-in-computer-vision-using-python-358d7c9863cb

HOG, LBP (texture), Color Histogram, 
SIFT, SURF (comp efficient surf), ORB 
Gabor (texture and edges)
Zernike  (shape representation)
Hu Moments (shape recognition)
Haralick Texture Features

keras applications
https://keras.io/api/applications/
https://keras.io/api/applications/HASHTAGusage-examples-for-image-classification-models

common feature extraction techniques include edge detection, color histograms, and texture analysis



\section{Choosing a Feature Extractor}
When selecting a feature extractor, several critical factors must be considered:
\begin{itemize}
    \item \textbf{Accuracy:} Accurate feature detection and description are crucial for navigation tasks to ensure precise localization and mapping. Inaccurate features can lead to errors in navigation and obstacle avoidance.
    \item \textbf{Speed:} Real-time processing necessitates fast computation and feature extraction to minimize latency, which is essential for timely decision-making in dynamic environments.
    \item \textbf{Robustness:} The feature extractor should exhibit invariance to changes in scale, rotation, illumination, and noise to ensure consistent performance across different flight conditions and environments.
    \item \textbf{Complexity:} Computational complexity directly impacts processing time and resource consumption, which is crucial for resource-constrained UAV systems where power efficiency is a consideration.
    \item \textbf{Scalability:} The algorithm should efficiently handle large datasets or high-resolution images, maintaining performance as the scale of data increases.
\end{itemize}

\section{Parameters Associated with Feature Extractors}
Feature extractors are characterized by several parameters that influence their performance:

\subsection{Descriptor Size}
The descriptor size, or the length of the feature vector, significantly impacts both the discriminative power and computational load of the feature extractor.
\begin{itemize}
    \item \textbf{Implementation:} Common descriptors include SIFT (128 dimensions), SURF (64 or 128 dimensions), and ORB (32 or 256 dimensions). Larger descriptors like those used in SIFT capture more detailed information about each keypoint.
    \item \textbf{Pros:} Larger descriptors are more discriminative and better at distinguishing between different keypoints, improving matching accuracy across varied conditions.
    \item \textbf{Cons:} Increased descriptor size results in higher computational costs for storage and matching, which can be a bottleneck in real-time applications. For instance, matching thousands of 128-dimensional descriptors can be significantly slower than matching 32-dimensional descriptors.
    \item \textbf{Impact:} The choice of descriptor size must balance the need for detail and accuracy against the available computational resources and real-time constraints.
\end{itemize}

Mathematically, if $d$ is the descriptor size and $N$ is the number of keypoints, the storage requirement is $O(N \times d)$. The matching complexity, assuming a brute-force approach, is $O(N^2 \times d)$. Hence, optimizing $d$ is crucial for real-time performance.

\subsection{Keypoint Detection Threshold}
The keypoint detection threshold determines the sensitivity of the feature detector to identifying keypoints in the image.
\begin{itemize}
    \item \textbf{Implementation:} In algorithms like SIFT and SURF, the threshold is used to filter out weak keypoints. ORB uses the FAST detector with a threshold to select keypoints.
    \item \textbf{Pros:} A lower threshold increases the number of detected keypoints, which enhances robustness and ensures that important features are not missed, especially in complex or textured scenes.
    \item \textbf{Cons:} Increasing the number of keypoints also increases computational demands, both in terms of memory usage and processing time for descriptor computation and matching.
    \item \textbf{Impact:} Selecting an appropriate threshold is crucial. Too high a threshold might miss important features, reducing robustness, while too low a threshold might result in an unmanageable number of keypoints, slowing down processing.
\end{itemize}

Mathematically, if $t$ is the threshold, then the number of keypoints $N$ can be modeled as a function $N = f(t)$. Lowering $t$ increases $N$, thus increasing the computational load $O(f(t) \times d)$.

\subsection{Octave Layers}
Octave layers refer to the levels in the scale-space pyramid used for multi-scale feature detection, enhancing robustness to scale variations.
\begin{itemize}
    \item \textbf{Implementation:} In SIFT, the image is repeatedly blurred and subsampled to create a scale-space pyramid with multiple octaves, each containing several layers.
    \item \textbf{Pros:} More octave layers improve the feature extractor's ability to detect features at various scales, making it robust to changes in object size and distance.
    \item \textbf{Cons:} More layers increase computational complexity and processing time. Each additional octave layer involves further blurring, subsampling, and keypoint detection steps.
    \item \textbf{Impact:} The number of octave layers must be chosen to balance scale invariance against computational efficiency. Insufficient layers might miss features at certain scales, while too many layers could slow down processing.
\end{itemize}

Mathematically, if $O$ is the number of octaves and $L$ is the number of layers per octave, the computational complexity for constructing the pyramid is $O(O \times L \times N)$, where $N$ is the number of pixels.

\subsection{Grid Size}
The grid size affects the granularity of feature extraction, impacting the detail and computational efficiency.
\begin{itemize}
    \item \textbf{Implementation:} Grid size refers to the spatial subdivision of the image into regions for feature extraction. Smaller grid sizes provide finer granularity.
    \item \textbf{Pros:} Finer grids capture more detailed features and spatial relationships, improving the robustness and accuracy of the feature descriptors, particularly in textured regions.
    \item \textbf{Cons:} Smaller grid sizes require more computational resources for both extraction and matching, as more regions need to be processed and compared.
    \item \textbf{Impact:} The grid size must be carefully selected based on the complexity of the scene and the available computational resources. Finer grids are beneficial for detailed scenes but may not be practical for real-time processing on resource-limited platforms.
\end{itemize}

If $G$ is the number of grid cells, then the computational complexity increases with $G$, as each cell needs to be processed independently, leading to a complexity of $O(G \times f(t) \times d)$.

\subsection{Orientation Assignment}
Orientation assignment ensures rotation invariance by assigning an orientation to each keypoint, which is crucial for consistent feature matching under different rotational perspectives.
\begin{itemize}
    \item \textbf{Implementation:} SIFT assigns an orientation based on the gradient direction around the keypoint. ORB uses intensity centroid methods for orientation assignment.
    \item \textbf{Pros:} Assigning orientations to keypoints makes the descriptors invariant to image rotations, improving matching accuracy when the image or object is viewed from different angles.
    \item \textbf{Cons:} The orientation assignment step adds to the computational complexity and processing time. Incorrect orientation assignment can degrade matching performance.
    \item \textbf{Impact:} Ensuring accurate and efficient orientation assignment is vital for applications involving rotational movements. It enhances the robustness of the feature extractor but needs to be balanced against the additional computational overhead.
\end{itemize}

If $\theta$ is the orientation angle, the computation involves determining $\theta$ for each keypoint, adding a complexity of $O(N)$ where $N$ is the number of keypoints.

\section{Types of Feature Extractors}
\subsection{Traditional Feature Extractors}
\subsubsection{SIFT (Scale-Invariant Feature Transform)}
SIFT detects and describes local features in images. It is robust to changes in scale, rotation, and illumination, making it a reliable choice for many applications. However, its computational intensity can be a drawback in real-time scenarios.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy and robustness due to its multi-scale approach and precise keypoint localization. SIFT's descriptors are highly distinctive, enabling reliable matching across different views and conditions.
    \item \textbf{Disadvantages:} High computational cost and slower processing speed due to the extensive keypoint detection and descriptor computation steps, making it less suitable for real-time applications.
\end{itemize}

Mathematically, SIFT's computational complexity is $O(O \times L \times N \times d)$ for the scale-space construction and $O(N \times d)$ for descriptor computation, where $O$ is the number of octaves, $L$ is the number of layers, $N$ is the number of keypoints, and $d$ is the descriptor size.

\subsubsection{SURF (Speeded-Up Robust Features)}
SURF is a faster alternative to SIFT, utilizing integral images for rapid computation of image convolutions. It offers good accuracy and robustness while being computationally more efficient than SIFT.
\begin{itemize}
    \item \textbf{Advantages:} Faster than SIFT due to its use of Haar wavelets and integral images, providing good balance between speed and accuracy. It maintains robustness to scale and rotation changes.
    \item \textbf{Disadvantages:} Still relatively computationally expensive compared to simpler methods like ORB, and can be less accurate than SIFT in certain complex scenarios.
\end{itemize}

Mathematically, SURF's computational complexity is reduced to $O(O \times N \times \log N)$ for the integral image computation and $O(N \times d)$ for descriptor computation.

\subsubsection{ORB (Oriented FAST and Rotated BRIEF)}
ORB combines the FAST keypoint detector and the BRIEF descriptor, providing a highly efficient feature extraction method suitable for real-time applications. It is designed to be both fast and invariant to rotation and scale.
\begin{itemize}
    \item \textbf{Advantages:} High speed and efficiency, making it suitable for real-time applications. ORB's binary descriptors are computationally less intensive while providing sufficient discriminative power for many tasks.
    \item \textbf{Disadvantages:} Lower accuracy compared to SIFT and SURF, especially in complex scenes with significant variations in lighting and scale. The binary nature of BRIEF descriptors can sometimes lead to higher false match rates.
\end{itemize}

Mathematically, ORB's complexity is $O(N \times \log N)$ for FAST keypoint detection and $O(N \times d)$ for BRIEF descriptor computation, where $d$ is typically smaller than in SIFT or SURF.

\subsection{Deep Learning-Based Feature Extractors}
\subsubsection{CNN-Based Extractors}
Convolutional Neural Networks (CNNs) have revolutionized feature extraction by learning feature representations directly from data. Models such as VGG, ResNet, and Inception have demonstrated high accuracy and robustness in various image processing tasks.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy and the ability to learn complex and hierarchical features directly from data, enabling robust performance across diverse tasks and conditions. CNNs can adapt to specific tasks through transfer learning.
    \item \textbf{Disadvantages:} Requires substantial computational resources for training and inference. The training process is data-intensive, often requiring large labeled datasets to achieve optimal performance.
\end{itemize}

Mathematically, CNNs involve multiple convolutional layers with a complexity of $O(N \times K^2 \times C)$ per layer, where $N$ is the number of pixels, $K$ is the kernel size, and $C$ is the number of channels. The total complexity depends on the depth and architecture of the network.

\subsubsection{Pre-trained Models}
Utilizing pre-trained models on large datasets like ImageNet can significantly expedite the development process and improve accuracy. These models can be fine-tuned for specific tasks, leveraging their learned representations.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy due to extensive pre-training on large and diverse datasets. Reduced training time and resource requirements during fine-tuning for specific applications.
    \item \textbf{Disadvantages:} May not generalize well to specific tasks without adequate fine-tuning. Potential for overfitting if the fine-tuning dataset is not representative of the target application.
\end{itemize}

Mathematically, the fine-tuning process involves updating a subset of weights, reducing the complexity to $O(M \times K^2 \times C)$, where $M$ is the number of modified weights, typically much smaller than the total number of weights.

\subsubsection{SuperPoint}
SuperPoint is a self-supervised framework for interest point detection and description, employing a fully convolutional neural network to detect points and describe them in a unified process. It is designed for real-time applications and offers a balance between accuracy and efficiency.
\begin{itemize}
    \item \textbf{Advantages:} High accuracy due to its end-to-end learning of keypoints and descriptors. It is robust to various transformations and efficient for real-time applications, leveraging deep learning techniques for feature extraction.
    \item \textbf{Disadvantages:} Requires substantial computational resources for training. While inference is efficient, domain-specific fine-tuning may still be necessary to achieve optimal performance in specialized tasks.
\end{itemize}

Mathematically, SuperPoint's complexity is similar to CNNs, with the additional step of keypoint extraction and description being integrated into a unified process, leading to an overall complexity of $O(N \times K^2 \times C)$ for inference.

\section{Evaluation for UAV-Based Navigation}
\subsection{Requirements}
For the Skripsie project, the feature extractor must:
\begin{itemize}
    \item Be capable of real-time processing to ensure timely navigation and decision-making in dynamic environments.
    \item Utilize the available high processing power of UAV systems efficiently, balancing accuracy and speed to maintain performance without excessive computational burden.
    \item Provide high accuracy to ensure reliable navigation, obstacle avoidance, and mapping, which are critical for UAV operations.
\end{itemize}

\subsection{Recommended Feature Extractor}
Given the requirements of real-time processing, high accuracy, and efficient utilization of processing power, a deep learning-based feature extractor is recommended. Particularly, a pre-trained CNN model such as ResNet or VGG, or a specialized model like SuperPoint, offers the best balance of these attributes. These models leverage the high processing power of modern UAV hardware, providing robust and accurate feature extraction essential for reliable navigation.

\section{Conclusion}
In conclusion, the choice of a feature extractor depends on the specific needs of the application. For UAV-based navigation in the Skripsie project, deep learning-based extractors, especially pre-trained models and SuperPoint, offer superior performance in terms of accuracy and real-time processing capabilities. These models provide a robust solution, leveraging advanced computational resources to meet the stringent demands of UAV navigation tasks.






\chapter{Results}
this table shows the st. dev. related to the estimation vs ground truth coordinates ratio. A more stable analysis implies a lower variation in the factor relating estimation (pixel change) to that of the true GPS coordinate change. A gaussian blur was applied with varying, square kernel sizes and the st. dev. was analysed.  
code as of 4aug (sift etc)


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Kernel Size} & \textbf{Standard Deviation X} & \textbf{Standard Deviation Y} & \textbf{Norm} \\
        \hline
        1   & 0.15985 & 0.03265 & 0.16317 \\
        3   & 0.16372 & 0.03547 & 0.16753 \\
        5   & 0.16166 & 0.03307 & 0.16401 \\
        11  & 0.17329 & 0.02227 & 0.17472 \\
        13  & 0.17310 & 0.03345 & 0.17630 \\
        15  & 0.17972 & 0.04036 & 0.18417 \\
        17  & 0.18370 & 0.05149 & 0.19177 \\
        19  & 0.15727 & 0.03997 & 0.16225 \\
        21  & 0.18859 & 0.03368 & 0.19159 \\
        23  & 0.15842 & 0.03414 & 0.16207 \\
        25  & 0.17395 & 0.03374 & 0.17720 \\
        27  & 0.15788 & 0.02341 & 0.15960 \\
        29  & 0.15515 & 0.03875 & 0.16095 \\
        41  & 0.15982 & 0.03484 & 0.16359 \\
        61  & 0.42036 & 0.08671 & 0.42918 \\
        91  & 1.22744 & 0.03476 & 1.22793 \\
        121 & 0.23488 & 0.12851 & 0.26695 \\
        181 & 1.22640 & 2.65690 & 2.93890 \\
        \hline
    \end{tabular}
    \caption{Standard Deviation and Norm of the Ratio of Actual to Estimated GPS Location for Various Kernel Sizes}
    \label{tab:stdev_norm}
\end{table}

A kernel size of 27 resulted in the lowest normalized standard deviation. It was subsequently made the primary use. A kernel size of 11 resulted in the lowest Y deviation, while a kernel size of one resulted in a fairly low deviation and ensures no important keypoints are removed. For this reason, those three kernel sizes will be tested with in future.

\subsection*{Comparison of SIFT with BFMatcher and SuperPoint with LightGlue for UAV Navigation}

In this analysis, we compare two feature extraction and matching techniques used for estimating GPS coordinates in UAV navigation. The first method utilizes the SIFT algorithm with BFMatcher, while the second employs SuperPoint with LightGlue. Both methods were applied to a dataset of aerial images, and the deviations between actual and estimated GPS coordinates were calculated. It's worth noting that the parameters for both methods were not optimized, which might affect their performance. The table below summarizes the results, showing the deviations in meters and the Euclidean norm for each method.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Image} & \textbf{Deviation X (m)} & \textbf{Deviation Y (m)} & \textbf{Norm (m)} \\
        \hline
        \multicolumn{4}{|c|}{\textbf{SIFT + BFMatcher}} \\
        \hline
        10 & 53.62 & 16.81 & 56.15 \\
        9 & 1287.48 & 5.02 & 1287.49 \\
        8 & 24.02 & 4.42 & 24.42 \\
        7 & 2.33 & 0.62 & 2.42 \\
        6 & 452.92 & 27.80 & 453.77 \\
        5 & 58.87 & 28.16 & 65.29 \\
        4 & 28.74 & 86.32 & 90.82 \\
        3 & 7.51 & 14.52 & 16.31 \\
        2 & 24.96 & 16.10 & 29.64 \\
        \hline
        \multicolumn{4}{|c|}{\textbf{SuperPoint + LightGlue}} \\
        \hline
        10 & 45.38 & 2.83 & 45.47 \\
        9 & 1281.47 & 41.44 & 1282.15 \\
        8 & 23.98 & 6.96 & 24.95 \\
        7 & 3.48 & 0.80 & 3.57 \\
        6 & 452.04 & 15.92 & 452.34 \\
        5 & 58.04 & 27.70 & 64.24 \\
        4 & 15.15 & 63.60 & 65.37 \\
        3 & 2.67 & 18.44 & 18.64 \\
        2 & 21.39 & 20.56 & 29.59 \\
        \hline
    \end{tabular}
    \caption{Comparison of Deviation and Norm for SIFT + BFMatcher and SuperPoint + LightGlue}
    \label{tab:comparison}
\end{table}

\subsection*{Analysis}
In this comparison, we observe that the SuperPoint + LightGlue combination provides lower deviation norms in 6 out of 9 cases for the X deviations, suggesting better performance in those instances. The unoptimized parameters may have affected the overall performance, but the results indicate that SuperPoint + LightGlue could offer more accurate estimates under the same conditions.


\subsection*{Percentile Outliers removal}


\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Lower Cutoff (\%)} & \textbf{Normalized Error} \\ \hline
0  & 59.57 \\ \hline
1  & 58.61 \\ \hline
2  & 57.71 \\ \hline
3  & 57.01 \\ \hline
4  & 55.33 \\ \hline
6  & 55.80 \\ \hline
10 & 54.96 \\ \hline
12 & 54.06 \\ \hline
14 & 54.00 \\ \hline
16 & 54.11 \\ \hline
18 & 53.98 \\ \hline
19 & 53.45 \\ \hline
20 & 53.82 \\ \hline
21 & 54.07 \\ \hline
22 & 54.44 \\ \hline
24 & 54.06 \\ \hline
26 & 54.00 \\ \hline
28 & 54.44 \\ \hline
30 & 53.66 \\ \hline
32 & 54.39 \\ \hline
34 & 54.14 \\ \hline
36 & 54.01 \\ \hline
38 & 53.49 \\ \hline
40 & 53.20 \\ \hline
41 & 53.46 \\ \hline
42 & 53.68 \\ \hline
43 & 53.82 \\ \hline
44 & 53.30 \\ \hline
45 & 52.87 \\ \hline
46 & 53.25 \\ \hline
47 & 52.48 \\ \hline
48 & 52.73 \\ \hline
49 & 53.74 \\ \hline
\end{tabular}
\caption{Normalized Error vs. Lower Percentile Cutoff}
\label{tab:normalized_error}
\end{table}


All parameter choices

1. feature extractor
2. Feature matcher
3. color or grayscale
4. Blur kernel size
5. percentile cutoff
6. outlier removal method
7. Amt of features to detect
8. Extractor threshold
9. How many matches to detect
10. Match threshold
11. Image size and resolution
12. Scaling factor for GPS to pixels
13. Speed / runtime
14. Accuracy   