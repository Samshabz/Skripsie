
Broadly speaking there are two types of feature matchers: local and global. Local matchers are used to find the relative pose between two images. Global matchers are used to find the correlation between two images.

\section*{Local Matchers}

\subsection*{Brute-Force Matcher (BFMatcher)} BFMatcher compares every descriptor from one image with all descriptors in another image to find the closest match. It calculates the distance between descriptors using a chosen metric (e.g., Euclidean for SIFT, Hamming for ORB). Matches are sorted by distance, and the best ones are selected. While simple and effective, it can be slow with large datasets. Best suited for smaller datasets or when precision is prioritized over speed.

\subsection*{FLANN (Fast Library for Approximate Nearest Neighbors)} FLANN is an efficient matcher that uses approximate nearest neighbor search, speeding up the matching process, especially for large datasets. It uses tree-based algorithms or k-means clustering to quickly find matches. FLANN is often preferred for tasks involving large datasets where speed is critical, such as real-time applications with SIFT or SURF descriptors. FLANN must be used with Local-sensitivity hashing to allow for operations on its binary descriptors. 

\subsection*{LightGlue} Light Glue is a lightweight, relative to other neural network approaches, neural network-based matcher that uses deep learning to match features across images. It is optimized for speed and memory efficiency, making it suitable for mobile and embedded systems. Light Glue provides robust matches even under challenging conditions, such as significant viewpoint changes or lighting variations. This is a significant improvement over traditional feature matching methods like BF or FLANN matchers, however, it increases computational time and risks an inability to generalize well, especially in this context which is unlikely to have training data based off UAV images. LightGlue works with Superpoint, and as such it is tested alongside SuperPoint in the feature extraction section.

\subsection*{SuperGlue} SuperGlue is a more advanced neural network-based matcher that leverages attention mechanisms, dynamically aggregating local features based on their inferred importance, to enhance the matching process. It works by learning to match keypoints directly from image data, providing superior performance in complex scenarios with large changes in scale, rotation, or perspective. SuperGlue is well-suited for high-precision applications like 3D reconstruction, but it cannot work in real-time due to its computational complexity.



\section*{Methodology}
- The extracted keypoints in both images are normalized to the global heading space True North from internal xy
- The extracted keypoints are matched using the chosen matcher
- The matches are filtered using Lowe's ratio test to remove ambiguous matches
- The matches are further refined using RANSAC to remove outliers
- The globally normalized (in NE space) matches are passed to the global matching technique for similarity computation or the local matcher for translation inference

\section*{Results}
what we test: we test overall accuracy and runtime differences. We do not test number of matches found as this does not inherently imply those matches are accurate. We also test robustness to different parameters. We also test robustness to different datasets. We also test robustness to different feature extraction methods. 
these tests are all done within the overall optimized methodology and tested with those parameters and methods which subtend the best accuracy while maintaining a good runtime.

options: bf with knn
- flann with knn
- bf with vanilla match
- bf with radius match
- lightglue is tested with superpoint seperately

vanilla match is poor due to no ambiguous match removal since it only returns a single match. It also generally has poor accuracy.

we are trying radius match - it works but it basically causes crap bc it only looks at descriptor space, we can do that sort anyway. we need the ambiguity removal of knn.
Anyways heres the results:



Percentage Deviation: [27.66623771] %
Preprocessing Global Detector: ORB, Preprocessing Global Matcher: BF, Global Matching Technique: Histogram, Local Detector: ORB, Local Matcher: BF
MAE GPS error: [216.8444969]
Mean normalized GPS error: [358.51765195]
 Mean Heading Error: 0.6756323096309731
Mean Number of Loc good Matches: 1190.888888888889
Mean Number of Global good Matches: 159.31465517241378
Time taken to execute The Method: 35.6156 seconds
End of Iteration: 1
Dataset: DATSETCPT
LEN MATCHES: 59

hard to tune nonsense radius matcher. tried tons of radii none subtend reasonable results. Pointless doing hyper optimization as it will not generalize well.


BF matcher test:


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Dataset} & \makecell{\textbf{RMSE GPS} \\ \textbf{(error)}} & \makecell{\textbf{MAE GPS} \\ \textbf{(error)}} & \makecell{\textbf{Runtime} \\ \textbf{(seconds)}} \\ 
        \hline
        \textbf{DATSETROT} & 75.78 & 53.02 & 37.38 \\ 
        \hline
        \textbf{DATSETCPT} & 18.12 & 12.51 & 43.65 \\ 
        \hline
        \textbf{DATSETROCK} & 28.10 & 19.84 & 33.56 \\ 
        \hline
        \textbf{DATSETSAND} & 357.19 & 249.82 & 41.07 \\ 
        \hline
        \textbf{DATSETAMAZ} & 76.30 & 51.86 & 48.42 \\ 
        \hline
    \end{tabular}
    \caption{BF Matcher RMSE, MAE, and Runtime Across Different Datasets}
\end{table}


\subsection*{Confidence Thresholding}



OPTIMIZATION by filtering non-ambiguous matches:





\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Threshold} & \multicolumn{2}{c|}{\textbf{AKAZE}} & \multicolumn{2}{c|}{\textbf{ORB}} \\ 
        \hline
        & \makecell{\textbf{RMSE GPS} \\ \textbf{(error)}} & \makecell{\textbf{Runtime} \\ \textbf{(seconds)}} & \makecell{\textbf{RMSE GPS} \\ \textbf{(error)}} & \makecell{\textbf{Runtime} \\ \textbf{(seconds)}} \\ 
        \hline
        \textbf{No Filter} & 61.64 & 42.43 & 46.76 & 35.73 \\ 
        \hline
        \textbf{1000} & 49.52 & 49.17 & 47.19 & 53.79 \\ 
        \hline
        \textbf{500} & 51.48 & 46.82 & 48.12 & 38.79 \\ 
        \hline
        \textbf{300} & 51.48 & 50.13 & 48.47 & 39.01 \\ 
        \hline
    \end{tabular}
    \caption{RMSE GPS Error and Runtime for AKAZE and ORB with Different Confidence Match Limits to Matches}
\end{table}

As visible, the AKAZE local detector is most accurate with a threshold of 1000, while the ORB local detector is most accurate with a threshold of 300. The runtime is also acceptable at both these thresholds. These were all run with AKAZE as the global detector and other parameters held constant, as with all these other tests. The important thing with these tests are the comparison not the other parameters.

Note: LightGlue and SuperPoint do not require such filtering, as their matches are inherently stronger and more reliable, negating the need for additional filtering steps, which turn out worse. 





\section*{Matching technique}
Different matchers use different algorithms to optimize the searching process for matches. For ORB and AKAZE, since the descriptors are binary, the only reliable, pre-made algorithm is knn matching which finds the best k matches. These are then filtered to a single match in additional steps (see Lowes ratio). 

\section*{Improvements to Matching techniques}
\subsection*{Cross-Check Matching} Cross-check matching performs matching in both directions (from image A to B and B to A) and only retains matches that are consistent in both directions. This reduces false positives and increases the reliability of matches. However, this doubles the computation time.

\subsection*{Lowe's ratio}
This technique involves comparing the distance of the best match to the distance of the second-best match. If the ratio is below a certain threshold, the match is considered valid. This helps filter out both low-quality and similar matches. 
0.8 if AKAZE, 0.6 if ORB. A higher threshold implies more ambiguous matches are included, while a lower threshold implies more matches are excluded. This implies AKAZE has more distinct and less ambiguous matches than ORB.

\subsection*{RANSAC (Random Sample Consensus)} RANSAC is used to refine matches by identifying and removing outliers. It works by iteratively selecting a subset of matches, estimating a model (like homography), and checking how well the remaining matches fit this model. Matches that deviate significantly are considered outliers and discarded. RANSAC is crucial in tasks like image stitching, where accurate geometric transformations are required. RANSAC performed optimally with 25 if AKAZE and 0.5 with ORB. This is because AKAZE contains more inliers and is more robust to geometric transformations.

Note, not running on optimizxed time or accuracy. make that clear at start. 

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Dataset} & \multicolumn{2}{c|}{\textbf{Runtime (seconds)}} & \multicolumn{2}{c|}{\textbf{RMSE GPS (error)}} \\
        \hline
        & \makecell{\textbf{Single and} \\ \textbf{Dual}} & \makecell{\textbf{Dual Only}} & \makecell{\textbf{Single and} \\ \textbf{Dual}} & \makecell{\textbf{Dual Only}} \\ 
        \hline
        \textbf{DATSETROT} & 71.75 & 50.62 & 68.27 & 68.27 \\ 
        \hline
        \textbf{DATSETCPT} & 52.96 & 57.54 & 22.34 & 19.44 \\ 
        \hline
        \textbf{DATSETROCK} & 59.62 & 65.59 & 27.32 & 27.78 \\ 
        \hline
        \textbf{DATSETSAND} & 65.95 & 57.25 & 159.03 & 159.82 \\ 
        \hline
        \textbf{DATSETAMAZ} & 58.61 & 49.99 & 126.99 & 123.09 \\ 
        \hline
    \end{tabular}
    \caption{FLANN Matcher with versus without single match inclusion}
\end{table}


FLANN shows large improvements in accuracy in the cases where there is significant difference in accuracy between the two methods. Further, runtime is generally improved when only including dual matches since generally less matches implies less time to process, apart from dynamic segment exclusions.

\subsection*{BF dual vs single }

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Dataset} & \multicolumn{2}{c|}{\textbf{Runtime (seconds)}} & \multicolumn{2}{c|}{\textbf{RMSE GPS (error)}} \\ 
        \hline
        & \makecell{\textbf{Single and} \\ \textbf{Dual}} & \makecell{\textbf{Dual Only}} & \makecell{\textbf{Single and} \\ \textbf{Dual}} & \makecell{\textbf{Dual Only}} \\ 
        \hline
        \textbf{DATSETROT} & 45.31 & 40.33 & 84.51 & 84.51 \\ 
        \hline
        \textbf{DATSETCPT} & 44.40 & 46.23 & 20.62 & 20.62 \\ 
        \hline
        \textbf{DATSETROCK} & 32.32 & 43.16 & 40.81 & 40.81 \\ 
        \hline
        \textbf{DATSETSAND} & 26.94 & 34.76 & 263.64 & 263.64 \\ 
        \hline
        \textbf{DATSETAMAZ} & 32.84 & 44.88 & 81.96 & 81.96 \\ 
        \hline
    \end{tabular}
    \caption{BF Matcher with versus without single match inclusion}
\end{table}


BF does not benefit from single match inclusion because it does not gain any single matches, they are all perfect dual matches. 


\subsection*{Conclusion}
In the above study we have looked in depth into BF and FLANN matchers. We have also looked into different matching and optimization techniques and how they can be used to improve the accuracy of the matches. We found that KNN match is the most reliable xyz method, in comparison to vanilla and radius match, due to its ability to be used with Lowe's threshold for  prevalent ambiguous matches. Further, we found using optimized Lowe's ratios, RANSAC, and Confidence thresholding not only improves accuracy but reduces processing time significantly. 